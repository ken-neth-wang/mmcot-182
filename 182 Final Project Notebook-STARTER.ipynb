{"cells":[{"cell_type":"markdown","metadata":{"id":"WihAr34xnxSz"},"source":["# Multi-modal Chain of Thought Model for ScienceQA problems "]},{"cell_type":"markdown","metadata":{"id":"4tShjtOFuJh1"},"source":["Chain of thought (CoT) refers to the mental process of reasoning and inference that humans use to arrive at an answer or solution to a problem. It involves synthesizing information from multiple sources, making logical connections between ideas, and integrating them into a coherent line of reasoning. In the context of natural language processing and deep learning models, CoT prompting involves generating intermediate reasoning steps to arrive at the final answer to a question. This process allows the model to break down complex questions into simpler, more manageable steps, and to leverage information from multiple sources, such as text and images, to arrive at a more accurate answer. This homework assignment is heavily adapted from this paper.\n","\n","This homework assignment will use the Science Question Answering (ScienceQA) dataset, comprises a total of 21,000+ multiple-choice science questions sourced from elementary and high school curricula. Through this notebook, you’ll explore a subset of this dataset, which consists of questions that only contain an a text context as well as questions that have both text and image contexts. \n","\n","This homework assignment will walk you through the sequential steps towards building a multimodal chain-of-thought model that utilizes both text and image inputs and chain-of-thought reasoning to solve ScienceQA problems. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"JEakJINJ8xIg"},"outputs":[],"source":["#@title Install dependencies - **Restart Runtime!!**\n","!pip install numpy==1.23.2\n","!pip install pandas==1.4.3\n","!pip install -U numpy\n","!pip install -U pandas\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rXwhMaUjb4_K"},"outputs":[],"source":["#@title Mount your Google Drive and Set up mount symlink\n","\n","import os\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","DRIVE_PATH = '/content/gdrive/MyDrive/cs182final_proj'\n","DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n","if not os.path.exists(DRIVE_PYTHON_PATH):\n","  %mkdir $DRIVE_PATH\n","\n","## the space in `My Drive` causes some issues,\n","## make a symlink to avoid this\n","SYM_PATH = '/content/cs182final_proj'\n","if not os.path.exists(SYM_PATH):\n","  !ln -s $DRIVE_PATH $SYM_PATH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_PQTBDkB9tSo","cellView":"form"},"outputs":[],"source":["#@title Clone homework repo\n","\n","%cd $SYM_PATH\n","if not os.path.exists(\"mm-cot\"):\n","  !git clone https://github.com/kevinjcai/mm-cot.git\n","else:\n","  %cd mm-cot\n","  !git pull\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FzwnYodJt97P","cellView":"form"},"outputs":[],"source":["#@title pip install \n","%cd mm-cot\n","!pip install -r req.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"jyz1EVW6o7Q7"},"outputs":[],"source":["#@title importing ntlk\n","import nltk\n","\n","# It will error the first time you run this cell\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"pLwFYacFLR7E"},"outputs":[],"source":["#@title download punkt \n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2W4DdqZmMzYM","cellView":"form"},"outputs":[],"source":["#@title import gdown\n","# Here we are using gdown to dowwnload model weights, images, and the vision features.\n","# The reason we do this is because these files are too large to put on github / take much \n","# longer. However, gdown sometimes fails or errors (out of our control), so if that is the case, \n","# please go to the link to manually download.\n","import gdown"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8LwXgJox_LOf"},"outputs":[],"source":["#@title Download pretrained model weights\n","\n","# If gdown fails or errors, go to the drive links directly and manually import the necessary files.\n","!gdown --id 1Em6GnwarLZxy0St9LEw6vKpAB4k7CXFt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zmpAVCWw24we"},"outputs":[],"source":["!tar -xvf experiments.tar.gz\n","!rm -rf experiments.tar.gz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rjTyHi2VSLvN"},"outputs":[],"source":["#@title Download images for dataset\n","# If gdown fails or errors, go to the drive links directly and manually import the necessary files.\n","\n","url = 'https://drive.google.com/drive/folders/1x7oPd31TPhWJowy3D4aGW2OsS6GVa2D5'\n","gdown.download_folder(url, quiet=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7EMRaU8MYbcu"},"outputs":[],"source":[" #@title Download vision features\n","\n","# If gdown fails or errors, go to the drive links directly and manually import the necessary files.\n","\n","# if not os.path.exists(\"vision_features\"):\n","%cd mm-cot\n","url = \"https://drive.google.com/drive/u/0/folders/10_DDp59tJwJafg6Kykg82GlZqQj_Irmz\"\n","gdown.download_folder(url, quiet=False, use_cookies=False)\n","%cd vision_features\n","!unzip -o /content/gdrive/MyDrive/cs182final_proj/mm-cot/vision_features/vision_features.zip\n","%cd ../\n","%pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"IL1guy0YtXan"},"outputs":[],"source":["#@title Download data \n","# cloning scienceqa dataset \n","#if not os.path.exists(\"data\"):\n","%cd mm-cot\n","!git clone https://github.com/lupantech/ScienceQA.git\n","!mv ScienceQA/data ./\n","!rm -r ScienceQA\n","# gdown.download_folder(url, quiet=False, use_cookies=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Evq0jF99GHah"},"outputs":[],"source":["#@title import statements\n","import sys\n","import os\n","import numpy as np\n","import torch\n","import os\n","import re\n","import json\n","import argparse\n","import random\n","from IPython.display import Image, display\n","from transformers import T5Tokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, T5ForConditionalGeneration\n","from utils_data import img_shape, load_data_std, load_data_img\n","from utils_prompt import *\n","from utils_evaluate import get_scores\n","from rich.table import Column, Table\n","from rich import box\n","from rich.console import Console\n","console = Console(record=True)\n","from torch import cuda\n","import nltk\n","import evaluate\n","import copy\n","\n","sys.path.append('/content/gdrive/MyDrive/182_mmcot')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AA1Xz2Nea21X"},"outputs":[],"source":["#@title Initialize args\n","# This creates the config arguments for the next part.\n","#import argparse\n","#argparse.ArgumentParser()\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('-f')\n","    parser.add_argument('--data_root', type=str, default='data')\n","    parser.add_argument('--output_dir', type=str, default='experiments')\n","    parser.add_argument('--model', type=str, default='allenai/unifiedqa-t5-base')\n","    parser.add_argument('--options', type=list, default=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n","    parser.add_argument('--epoch', type=int, default=1)\n","    parser.add_argument('--lr', type=float, default=5e-5)\n","    parser.add_argument('--bs', type=int, default=1)\n","    parser.add_argument('--input_len', type=int, default=512)\n","    parser.add_argument('--output_len', type=int, default=64)\n","    parser.add_argument('--eval_bs', type=int, default=1)\n","    parser.add_argument('--eval_acc', type=int, default=None, help='evaluate accumulation step')\n","    parser.add_argument('--train_split', type=str, default='minitrain', choices=['train', 'trainval', 'minitrain'])\n","    parser.add_argument('--val_split', type=str, default='minival', choices=['test', 'val', 'minival'])\n","    parser.add_argument('--test_split', type=str, default='minitest', choices=['test', 'minitest'])\n","    \n","    parser.add_argument('--use_generate', action='store_true', help='only for baseline to improve inference speed')\n","    parser.add_argument('--final_eval', action='store_true', help='only evaluate the model at the final epoch')\n","    parser.add_argument('--user_msg', type=str, default=\"baseline\", help='experiment type in the save_dir')\n","    parser.add_argument('--img_type', type=str, default=None, choices=['detr', 'clip', 'resnet'], help='type of image features')\n","    parser.add_argument('--eval_le', type=str, default=None, help='generated rationale for the dev set')\n","    parser.add_argument('--test_le', type=str, default=None, help='generated rationale for the test set')\n","    parser.add_argument('--evaluate_dir', type=str, default=None, help='the directory of model for evaluation')\n","    parser.add_argument('--caption_file', type=str, default='data/captions.json')\n","    parser.add_argument('--use_caption', action='store_true', help='use image captions or not')\n","    parser.add_argument('--prompt_format', type=str, default='QCM-A', help='prompt format template',\n","                        choices=['QCM-A', 'QCM-LE', 'QCMG-A', 'QCM-LEA', 'QCM-ALE'])\n","    parser.add_argument('--seed', type=int, default=42, help='random seed')\n","\n","    args = parser.parse_args()\n","    return args\n","args = parse_args()"]},{"cell_type":"markdown","metadata":{"id":"9OTn179rtUAz"},"source":["# a) Prompt Building\n","\n","Prompt building is commonly used in tasks such as text completion, translation, and question answering, where the model is required to generate output that is consistent with a given input prompt. Prompt building can be a highly effective technique for improving the accuracy and performance of language models, as it allows us to fine-tune the model's behavior for specific tasks and domains. Additionally, prompt building can help to mitigate the problem of bias in language models, as it provides a way to explicitly specify the desired output and constrain the model's behavior.\n"]},{"cell_type":"markdown","metadata":{"id":"SztFNXQSvhiX"},"source":["This block will show exactly one problem and the associated text data. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QdZePNHwo0Jb"},"outputs":[],"source":["# 'problems' stores the raw json file that contains data. \n","# This block will show exactly one problem and the associated text data. \n","problems = json.load(open('/content/cs182final_proj/mm-cot/data/scienceqa/problems.json'))\n","problems['7']"]},{"cell_type":"markdown","metadata":{"id":"Vybzl_cbpG42"},"source":["The following block prints out the question text as well as the context. In the raw json file, the `hint` corresponds to the question context."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U0r3v27Koowx"},"outputs":[],"source":["index = 7 ### FEEL FREE TO TRY CHANGING THE INDEX TO GET DIFFERENT QUESTIONS from the dataset\n","\n","sampleProb = (problems[str(index)])\n","print(f\"Question Text for Problem # {index}: \" + get_question_text(sampleProb))\n","print(f\"Question Context for Problem # {index}: \" + get_context_text(sampleProb, False))\n","print(f\"Answer Choices for Problem # {index}: \" + get_choice_text(sampleProb, args.options))\n","# Lecture text isn't used in this model. However, you can explore more of the scienceQA dataset \n","# at this link https://scienceqa.github.io/explore.html \n","print(f\"Lecture Text for Problem # {index}: \" + get_lecture_text(sampleProb))\n"]},{"cell_type":"markdown","metadata":{"id":"PsdUpR26pyO6"},"source":["# a) i. \n","\n","This chain-of-thought model uses a two-stage framework. The first stage generates the rationale, which is trained with the \"solution\" text as the target. Run the block to see what the \"solution\" corresponds to for the question you saw above. How might this solution help the second model to get the answer?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iwq0BYpCpjKj"},"outputs":[],"source":["get_solution_text(sampleProb)"]},{"cell_type":"markdown","metadata":{"id":"A7cNRsO1v1AV"},"source":["# a) ii. \n","What are the components of the input prompt to the first stage? In other words, which pieces of a specific datapoint in the ScienceQA dataset do you concatenate together to generate the input prompt? Write this out and maintain the order that each component is attached. \n","hint: Look at build_train_pair and the following blocks below. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VoAt1sZ0fLQ1"},"outputs":[],"source":["index = 7\n","\n","shot_qids = ['1', '10']\n","test_qid = str(index)\n","\n","input, target = build_train_pair(problems, test_qid, args)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kI0FKzppZdig"},"outputs":[],"source":["input"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jT2M_d99Ze_r"},"outputs":[],"source":["target"]},{"cell_type":"markdown","metadata":{"id":"sE-ZbwRzniJQ"},"source":["# a) ii. Conceptual Question: `build_train_pair` creates the prompt that represents the exact text input into the model. Write out the exact components of this prompt in the right order (ie. the raw dataset consists of different components such as the question text, the choices text, etc. How does `build_train_pair` actually build the prompt that serves as the input?)"]},{"cell_type":"markdown","metadata":{"id":"IWtxvZe8qsVr"},"source":["Feel free to write your answer here."]},{"cell_type":"markdown","metadata":{"id":"oyHAl4wWp5bM"},"source":["# b) Add images\n"]},{"cell_type":"markdown","metadata":{"id":"4uBeZQ-onVAG"},"source":["Note that not all questions are associated with an image - the ScienceQA dataset comprises 10,332 (48.7%) questions with an image context, 10,220 (48.2%) with a text context, and 6,532 (30.8%) with both modalities.\n"]},{"cell_type":"markdown","metadata":{"id":"ZuH-SN2cwJ_t"},"source":["# b) i. \n","Run the following cells You can try other indices and see the images as well as the questions they correspond to. To save space, this notebook only downloads certain images (question indices that produce an image are 7, 28, 45, 60). Notice that some indices produce only one image corresponding to the overall question, while other questions also produce images that correspond to the answers. Does adding the picture(s) make the question easier to solve? How might this inform our model?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZgiavNknK-9"},"outputs":[],"source":["index = 7\n","sampleProb = (problems[str(index)])\n","print(get_question_text(sampleProb) + \"\\n\")\n","print()\n","path = '/content/cs182final_proj/mm-cot/image_vals/' + str(index) + '_'\n","#NOTE: The image choice number (ie. choice 0 and choice 1) do NOT correspond to \n","# choice A, choice B (for instance, in this example, A is the discus, \n","#B is the armored catfish, but the discus image is labeled choice_1, the armored \n","# catfish image is labeled choice_0).\n","#This is an artifact of the ScienceQA dataset. For our purposes, this won't\n","#impact things, as we are simply showing the images to help build intuition. \n","#In this notebook, we will be using the vision features that have already been \n","# extracted from the images, and so won't be using the raw image data.\n","if os.path.isfile(path + 'choice_0.png'):\n","  display(Image(path + 'choice_0.png'))\n","print(\"\\n\")\n","if os.path.isfile(path + 'choice_1.png'):\n","  display(Image(path + 'choice_1.png'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NxsTeh3zpC9G"},"outputs":[],"source":["# For context, here is the answer that you should have gotten. \n","# Is it easier to get to this answer now that you've seen the image?\n","print(get_answer(sampleProb, args.options))\n","print(get_origin_answer(sampleProb, args.options))"]},{"cell_type":"markdown","metadata":{"id":"KF4yTN4-qSji"},"source":["# (c) Dataloading\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5K5__4DfuRtD","cellView":"form"},"outputs":[],"source":["#@title Set arguments (for rationale generation later)\n","args.model  = \"allenai/unifiedqa-t5-small\"\n","args.user_msg = \"rationale\"\n","args.img_type = \"detr\"\n","args.bs = 1\n","args.eval_bs = 1\n","args.eval_acc = 10\n","args.output_len = 512\n","args.final_eval = True\n","args.prompt_format = \"QCM-LE\"\n","args_rationale = copy.deepcopy(args)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E8kBftTstQKV"},"outputs":[],"source":["from torch.utils.data import Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"i0Dr7ZBk9Mk8"},"outputs":[],"source":["#@title Setup for dataset\n","import os\n","import json\n","import numpy as np\n","import torch\n","from utils_prompt import *\n","\n","img_shape = {\n","    \"resnet\": (512, 2048),\n","    \"clip\": (49, 2048),\n","    \"detr\": (100, 256),\n","}\n","\n","def load_data_std(args):\n","    problems = json.load(open(os.path.join(args.data_root, 'scienceqa/problems.json')))\n","    pid_splits = json.load(open(os.path.join(args.data_root, 'scienceqa/pid_splits.json')))\n","    captions = json.load(open(args.caption_file))[\"captions\"]\n","\n","    for qid in problems:\n","        problems[qid]['caption'] = captions[qid] if qid in captions else \"\"\n","\n","    train_qids = pid_splits['%s' % (args.train_split)]\n","    val_qids = pid_splits['%s' % (args.val_split)]\n","    test_qids = pid_splits['%s' % (args.test_split)]\n","    print(f\"number of train problems: {len(train_qids)}\\n\")\n","    print(f\"number of val problems: {len(val_qids)}\\n\")\n","    print(f\"number of test problems: {len(test_qids)}\\n\")\n","\n","    qids = {'train': train_qids, 'val':val_qids,'test':test_qids}\n","    return problems, qids,\n","\n","def load_data_img(args):\n","    problems = json.load(open(os.path.join(args.data_root, 'scienceqa/problems.json')))\n","    pid_splits = json.load(open(os.path.join(args.data_root, 'scienceqa/pid_splits.json')))\n","    captions = json.load(open(args.caption_file))[\"captions\"]\n","    name_maps = json.load(open('vision_features/name_map.json'))\n","\n","    # check\n","    if args.img_type == \"resnet\":\n","        image_features = np.load('vision_features/resnet.npy')\n","        image_features = np.expand_dims(image_features, axis=1)\n","        image_features = image_features.repeat(512, axis=1)\n","    elif args.img_type == \"clip\":\n","        image_features = np.load('vision_features/clip.npy')\n","    elif args.img_type == \"detr\":\n","        image_features = np.load('vision_features/detr.npy')\n","    else:\n","        image_features = np.load('vision_features/detr.npy')\n","    print(\"img_features size: \", image_features.shape)\n","\n","    for qid in problems:\n","        problems[qid]['caption'] = captions[qid] if qid in captions else \"\"\n","\n","    train_qids = pid_splits['%s' % (args.train_split)]\n","    val_qids = pid_splits['%s' % (args.val_split)]\n","    test_qids = pid_splits['%s' % (args.test_split)]\n","    print(f\"number of train problems: {len(train_qids)}\\n\")\n","    print(f\"number of val problems: {len(val_qids)}\\n\")\n","    print(f\"number of test problems: {len(test_qids)}\\n\")\n","\n","    qids = {'train': train_qids, 'val':val_qids,'test':test_qids}\n","    return problems, qids, name_maps, image_features\n","\n","class ScienceQADatasetStd(Dataset):\n","    \"\"\"\n","    Creating a custom dataset for reading the dataset and\n","    loading it into the dataloader to pass it to the\n","    neural network for finetuning the model\n","\n","    \"\"\"\n","\n","    def __init__(\n","        self, problems, qids, tokenizer, source_len, target_len, args, test_le=None\n","    ):\n","        self.tokenizer = tokenizer\n","        self.data = {qid : problems[qid] for qid in qids}\n","        self.source_len = source_len\n","        self.summ_len = target_len\n","        self.target_text = []\n","        self.source_text = []\n","        if test_le is not None:\n","            test_le_data =json.load(open(test_le))[\"preds\"]\n","        else:\n","            test_le_data = None\n","        idx = 0\n","        for qid in self.data:\n","            if test_le_data is not None:\n","                curr_le_data = test_le_data[idx]\n","                idx += 1\n","            else:\n","                curr_le_data = None\n","            prompt, target = build_train_pair(problems, qid, args, curr_le_data)\n","            self.target_text.append(target)\n","            self.source_text.append(prompt)\n","\n","    def __len__(self):\n","        return len(self.target_text)\n","\n","    def __getitem__(self, index):\n","        source_text = str(self.source_text[index])\n","        target_text = str(self.target_text[index])\n","\n","        # cleaning data so as to ensure data is in string type\n","        source_text = \" \".join(source_text.split())\n","        target_text = \" \".join(target_text.split())\n","\n","        source = self.tokenizer.batch_encode_plus(\n","            [source_text],\n","            max_length=self.source_len,\n","            pad_to_max_length=True,\n","            truncation=True,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\",\n","        )\n","        target = self.tokenizer.batch_encode_plus(\n","            [target_text],\n","            max_length=self.summ_len,\n","            pad_to_max_length=True,\n","            truncation=True,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\",\n","        )\n","        source_ids = source[\"input_ids\"].squeeze()\n","        source_mask = source[\"attention_mask\"].squeeze()\n","        target_ids = target[\"input_ids\"].squeeze().tolist()\n","        \n","        return {\n","            \"input_ids\": source_ids,\n","            \"attention_mask\": source_mask,\n","            \"labels\": target_ids,\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"IKXMtM_o2677"},"outputs":[],"source":["#@title Load data and image features\n","# training logger to log training progress\n","training_logger = Table(\n","    Column(\"Epoch\", justify=\"center\"),\n","    Column(\"Steps\", justify=\"center\"),\n","    Column(\"Loss\", justify=\"center\"),\n","    title=\"Training Status\",\n","    pad_edge=False,\n","    box=box.ASCII,\n",")\n","\n","#args = parse_args()\n","print(\"args\",args)\n","print('====Input Arguments====')\n","print(json.dumps(vars(args), indent=2, sort_keys=False))\n","\n","random.seed(args.seed)\n","\n","if not os.path.exists(args.output_dir):\n","        os.mkdir(args.output_dir)\n","\n","if args.img_type is not None:\n","    problems, qids, name_maps, image_features = load_data_img(args)  # problems, test question ids, shot example ids\n","    dataframe = {'problems':problems, 'qids':qids, 'name_maps': name_maps, 'image_features': image_features}\n","else:\n","    problems, qids = load_data_std(args)  # problems, test question ids, shot example ids\n","    dataframe = {'problems':problems, 'qids':qids}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yeWoeE0orhC8"},"outputs":[],"source":["#@title Split qids\n","torch.manual_seed(args.seed)  # pytorch random seed\n","np.random.seed(args.seed)  # numpy random seed\n","torch.backends.cudnn.deterministic = True\n","\n","if args.evaluate_dir is not None:\n","    args.model = args.evaluate_dir\n","\n","tokenizer = T5Tokenizer.from_pretrained(args.model)\n","\n","console.log(f\"\"\"[Model]: Loading {args.model}...\\n\"\"\")\n","console.log(f\"[Data]: Reading data...\\n\")\n","problems = dataframe['problems']\n","qids = dataframe['qids']\n","train_qids = qids['train']\n","test_qids = qids['test']\n","val_qids = qids['val']\n","\n","if args.evaluate_dir is not None:\n","    save_dir = args.evaluate_dir\n","else:\n","    model_name = args.model.replace(\"/\",\"-\")\n","    gpu_count = torch.cuda.device_count()\n","    save_dir = f\"{args.output_dir}/{args.user_msg}_{model_name}_{args.img_type}_{args.prompt_format}_lr{args.lr}_bs{args.bs * gpu_count}_op{args.output_len}_ep{args.epoch}\"\n","    if not os.path.exists(save_dir):\n","        os.mkdir(save_dir)\n","\n","padding_idx = tokenizer._convert_token_to_id(tokenizer.pad_token)"]},{"cell_type":"markdown","metadata":{"id":"niQQPNfBxCSJ"},"source":["# c) Dataloading \n","### CODING PART BELOW \n","Dataloading typically involves reading data from one or more sources, such as a file or a database, and performing preprocessing steps such as normalization, transformation, or augmentation, in order to prepare the data for use in the model.This process determines how efficiently and effectively the model can learn from the data. \n"]},{"cell_type":"markdown","metadata":{"id":"QepZXFYcxSCx"},"source":["i. Implement the following part. \n","\n","Initialize and tokenize the prompt and the target. \n","\n","Run the “Dataloader Test” code cell to check your answer.\n","\n","*Hint: to initialize the prompt and the target, use the given prompt building utils.*\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3BCScfkjxX72"},"outputs":[],"source":["class ScienceQADatasetImg(Dataset):\n","    \"\"\"\n","    Creating a custom dataset for reading the dataset and\n","    loading it into the dataloader to pass it to the\n","    neural network for finetuning the model\n","    \"\"\"\n","\n","    def __init__(\n","        self, problems, qids, name_maps, tokenizer, source_len, target_len, args, image_features, test_le=None\n","    ):\n","        \"\"\"\n","        Initializes a Dataset class\n","        Args:\n","            dataframe (pandas.DataFrame): Input dataframe\n","            tokenizer (transformers.tokenizer): Transformers tokenizer\n","            source_len (int): Max length of source text\n","            target_len (int): Max length of target text\n","            source_text (str): column name of source text\n","            target_text (str): column name of target text\n","        \"\"\"\n","        self.tokenizer = tokenizer\n","        self.data = {qid : problems[qid] for qid in qids}\n","        self.source_len = source_len\n","        self.summ_len = target_len\n","        self.target_text = []\n","        self.source_text = []\n","        self.image_ids = []\n","        if test_le is not None:\n","            test_le_data =json.load(open(test_le))[\"preds\"]\n","        else:\n","            test_le_data = None\n","        idx = 0\n","        for qid in self.data:\n","            if test_le_data is not None:\n","                curr_le_data = test_le_data[idx]\n","                idx += 1\n","            else:\n","                curr_le_data = None\n","\n","\n","            ### TO DO ###\n","\n","            ### HINT: Look at utils_prompt.py ###\n","\n","            ###HINT: Note that you want to be building the target and the prompt###\n","            prompt, target = build_train_pair(problems, qid, args, curr_le_data)\n","            self.target_text.append(target)\n","            self.source_text.append(prompt)\n","            #############\n","            if str(qid) in name_maps:\n","                i_vectors = image_features[int(name_maps[str(qid)])]\n","                self.image_ids.append(i_vectors)\n","            else:\n","                shape = img_shape[args.img_type]\n","                self.image_ids.append(np.zeros(shape))\n","    \n","    def __len__(self):\n","        \"\"\"returns the length of dataframe\"\"\"\n","\n","        return len(self.target_text)\n","\n","    def __getitem__(self, index):\n","        \"\"\"return the input ids, attention masks and target ids\"\"\"\n","\n","        source_text = str(self.source_text[index])\n","        target_text = str(self.target_text[index])\n","        image_ids = self.image_ids[index]\n","\n","        # cleaning data so as to ensure data is in string type\n","        source_text = \" \".join(source_text.split())\n","        target_text = \" \".join(target_text.split())\n","\n","        ### TO DO: Implement tokenizer. Documentation here: \n","        ### https://huggingface.co/transformers/v2.11.0/main_classes/tokenizer.html \n","        ### HINT: You'll want to tokenize batches \n","        source = self.tokenizer.batch_encode_plus(\n","            [source_text],\n","            max_length=self.source_len,\n","            pad_to_max_length=True,\n","            truncation=True,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\",\n","        )\n","        target = self.tokenizer.batch_encode_plus(\n","            [target_text],\n","            max_length=self.summ_len,\n","            pad_to_max_length=True,\n","            truncation=True,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\",\n","        )\n","        #############\n","        source_ids = source[\"input_ids\"].squeeze()\n","        source_mask = source[\"attention_mask\"].squeeze()\n","        target_ids = target[\"input_ids\"].squeeze().tolist()\n","\n","        image_ids = torch.tensor(image_ids).squeeze()\n","        \n","        return {\n","            \"input_ids\": source_ids,\n","            \"attention_mask\": source_mask,\n","            \"image_ids\": image_ids,\n","            \"labels\": target_ids,\n","        }"]},{"cell_type":"markdown","metadata":{"id":"izAG2k_JrGe-"},"source":["# Test whether your data loader works"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8McqrsqHq8Cv"},"outputs":[],"source":["#@title Create Dataloader\n","patch_size = img_shape[args.img_type]\n","#model = T5ForMultimodalGeneration.from_pretrained(args.model, patch_size=patch_size, padding_idx=padding_idx, save_dir=save_dir)\n","name_maps = dataframe['name_maps']\n","image_features = dataframe['image_features']\n","train_set = ScienceQADatasetImg(\n","    problems,\n","    train_qids,\n","    name_maps,\n","    tokenizer,\n","    args.input_len,\n","    args.output_len,\n","    args,\n","    image_features,\n",")\n","eval_set = ScienceQADatasetImg(\n","    problems,\n","    val_qids[:20],\n","    name_maps,\n","    tokenizer,\n","    args.input_len,\n","    args.output_len,\n","    args,\n","    image_features,\n","    args.eval_le,\n",")\n","test_set = ScienceQADatasetImg(\n","    problems,\n","    test_qids[:20],\n","    name_maps,\n","    tokenizer,\n","    args.input_len,\n","    args.output_len,\n","    args,\n","    image_features,\n","    args.test_le,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uDUitkUw-OUm"},"outputs":[],"source":["#@title Test Dataloader\n","# test train set\n","expected_input_ids = np.array([11860,    10,  4073,    13,   175,  2315,    19,   623,   189,   222,\n","         3414,    58,  1193,  6327,    10,   445,    87,   188, 17011,    10,\n","           41,   188,    61,  1013,  5089,    41,   279,    61, 17903,  2834,\n","           41,   254,    61, 10585,    41,   308,    61, 13782, 17942,    10,\n","            1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0])\n","assert np.allclose(np.array(train_set[7]['input_ids']), expected_input_ids)\n"]},{"cell_type":"markdown","metadata":{"id":"UP4EUiPDta_L"},"source":["# d) Model architecture "]},{"cell_type":"markdown","metadata":{"id":"2czXLv4wydJy"},"source":["i. The two-stage framework takes the input (text and image), generates rationale, and then appends the rationale to the original input to create a modified input. This modified input is then passed into the second model, the inference model. What architectural structure (covered in the course) is this reminiscent of or analogous to?"]},{"cell_type":"markdown","metadata":{"id":"VlYD9sQPxXAA"},"source":["ii. Refer to the Homework PDF for information on the model architecture. "]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"5_jzn5MrA4U9"},"outputs":[],"source":["#@title Metrics\n","def extract_ans(ans):\n","    pattern = re.compile(r'The answer is \\(([A-Z])\\)')\n","    res = pattern.findall(ans)\n","    \n","    if len(res) == 1:\n","        answer = res[0]  # 'A', 'B', ...\n","    else:\n","        answer = \"FAILED\" \n","    return answer  \n","\n","# accuracy for answer inference\n","def compute_metrics_acc(eval_preds):\n","    if args.use_generate:\n","        preds, targets = eval_preds\n","        if isinstance(preds, tuple):\n","            preds = preds[0]\n","    else:\n","        preds = eval_preds.predictions[0]\n","        targets = eval_preds.label_ids\n","        preds = preds.argmax(axis=2)\n","    preds = tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","    targets = tokenizer.batch_decode(targets, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","    correct = 0\n","    assert len(preds) == len(targets)\n","    for idx, pred in enumerate(preds):\n","        reference = targets[idx]\n","        reference = extract_ans(reference)\n","        extract_pred = extract_ans(pred)\n","        best_option = extract_pred\n","        if reference == best_option:\n","            correct +=1 \n","    return {'accuracy': 1.0*correct/len(targets)}\n","\n","# rougel for rationale generation\n","metric = evaluate.load(\"rouge\")\n","def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [label.strip() for label in labels]\n","    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n","    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n","    return preds, labels\n","\n","def compute_metrics_rougel(eval_preds):\n","    if args.use_generate:\n","        preds, targets = eval_preds\n","        if isinstance(preds, tuple):\n","            preds = preds[0]\n","    else:\n","        preds = eval_preds.predictions[0]\n","        targets = eval_preds.label_ids\n","        preds = preds.argmax(axis=2)\n","    preds = tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","    targets = tokenizer.batch_decode(targets, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","\n","    decoded_preds, decoded_labels = postprocess_text(preds, targets)\n","\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    result = {k: round(v * 100, 4) for k, v in result.items()}\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    return result"]},{"cell_type":"markdown","metadata":{"id":"yJgovrvHBjUB"},"source":["# Model building here \n","### CODING PART HERE (implement the TODO sections). Refer to the PDF. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rT7z_vbuBiBg"},"outputs":[],"source":["# @title Model Class (TODO)\n","from transformers import T5Config, T5ForConditionalGeneration\n","from transformers.models.t5.modeling_t5 import T5Stack, __HEAD_MASK_WARNING_MSG, T5EncoderModel\n","import copy\n","import math\n","import os\n","import warnings\n","from typing import Optional, Tuple, Union\n","import torch\n","from torch import nn\n","from torch.nn import CrossEntropyLoss\n","from transformers.modeling_outputs import (\n","    BaseModelOutput,\n","    Seq2SeqLMOutput,\n",")\n","\n","class T5ForMultimodalGeneration(T5ForConditionalGeneration):\n","    _keys_to_ignore_on_load_missing = [\n","        r\"encoder.embed_tokens.weight\",\n","        r\"decoder.embed_tokens.weight\",\n","        r\"lm_head.weight\",\n","    ]\n","    _keys_to_ignore_on_load_unexpected = [\n","        r\"decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight\",\n","    ]\n","\n","    def __init__(self, config: T5Config, patch_size, padding_idx, save_dir):\n","        super().__init__(config)\n","        self.model_dim = config.d_model\n","        \n","        self.padding_idx = padding_idx\n","        self.out = open(os.path.join(save_dir, 'gate.txt'), 'w')\n","\n","\n","        ## TODO IMPLEMENT MODEL HERE \n","        # HINT START WITH EMBEDDING\n","        self.shared = \n","        self.patch_num, self.patch_dim = \n","\n","        self.image_dense = \n","        self.mha_layer = \n","        self.gate_dense = \n","        self.sigmoid = \n","        ########################\n","\n","        ## comment the encoder and decoder \n","        encoder_config = copy.deepcopy(config)\n","        encoder_config.is_decoder = False\n","        encoder_config.use_cache = False\n","        encoder_config.is_encoder_decoder = False\n","\n","        ## TODO: Impelement encoder. Hint:  use t5stack\n","        #T5stack uses a stack of T5 encoders and decoders to improve \n","        #performance on tasks that require multi-step reasoning and inference. \n","        #The input text is first processed by a stack of T5 encoders,\n","        #which encode the input text into a series of feature vectors that capture its meaning and context. \n","        #The encoded features are then passed through a stack of T5 decoders, \n","        #which generate the output text by predicting the next token in the sequence at each step.\n","        self.encoder = \n","        ####################################\n","\n","        decoder_config = copy.deepcopy(config)\n","        decoder_config.is_decoder = True\n","        decoder_config.is_encoder_decoder = False\n","        decoder_config.num_layers = config.num_decoder_layers\n","        ################################\n","\n","        ## TODO: Impelement decoder. Hint:  use t5stack\n","        self.decoder = \n","        ############################\n","\n","        ## TODO: implement the lm_head layer\n","        self.lm_head = \n","\n","        ######################################################\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","        # Model parallel\n","        self.model_parallel = False\n","        self.device_map = None\n","\n","    def forward(\n","        self,\n","        input_ids: Optional[torch.LongTensor] = None,\n","        image_ids=None,\n","        attention_mask: Optional[torch.FloatTensor] = None,\n","        decoder_input_ids: Optional[torch.LongTensor] = None,\n","        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n","        head_mask: Optional[torch.FloatTensor] = None,\n","        decoder_head_mask: Optional[torch.FloatTensor] = None,\n","        cross_attn_head_mask: Optional[torch.Tensor] = None,\n","        encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n","        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n","        inputs_embeds: Optional[torch.FloatTensor] = None,\n","        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n","        labels: Optional[torch.LongTensor] = None,\n","        use_cache: Optional[bool] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n","        use_cache = use_cache if use_cache is not None else self.config.use_cache\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n","        if head_mask is not None and decoder_head_mask is None:\n","            if self.config.num_layers == self.config.num_decoder_layers:\n","                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)\n","                decoder_head_mask = head_mask\n","\n","        # Encode if needed (training, first prediction pass)\n","        if encoder_outputs is None:\n","            # Convert encoder inputs in embeddings if needed\n","            encoder_outputs = self.encoder(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                inputs_embeds=inputs_embeds,\n","                head_mask=head_mask,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","            )\n","\n","        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n","            encoder_outputs = BaseModelOutput(\n","                last_hidden_state=encoder_outputs[0],\n","                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n","                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n","            )\n","\n","        hidden_states = encoder_outputs[0]\n","        \n","\n","        ## TODO IMPLEMENT THE FORWARD FUNCTION\n","        image_embedding = \n","        image_att, _ = \n","\n","        merge = \n","        gate = \n","        hidden_states = \n","        ############################################\n","\n","        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n","            # get decoder inputs from shifting lm labels to the right\n","            decoder_input_ids = self._shift_right(labels)\n","\n","        \n","        # Decode\n","        decoder_outputs = self.decoder(\n","            input_ids=decoder_input_ids,\n","            attention_mask=decoder_attention_mask,\n","            inputs_embeds=decoder_inputs_embeds,\n","            past_key_values=past_key_values,\n","            encoder_hidden_states=hidden_states,\n","            encoder_attention_mask=attention_mask,\n","            head_mask=decoder_head_mask,\n","            cross_attn_head_mask=cross_attn_head_mask,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = decoder_outputs[0]\n","\n","        if self.config.tie_word_embeddings:\n","            # Rescale output before projecting on vocab\n","            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n","            sequence_output = sequence_output * (self.model_dim**-0.5)\n","\n","        lm_logits = self.lm_head(sequence_output)\n","\n","        loss = None\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss(ignore_index=-100)\n","            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n","\n","        if not return_dict:\n","            output = (lm_logits,) + decoder_outputs[1:] + encoder_outputs\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return Seq2SeqLMOutput(\n","            loss=loss,\n","            logits=lm_logits,\n","            past_key_values=decoder_outputs.past_key_values,\n","            decoder_hidden_states=decoder_outputs.hidden_states,\n","            decoder_attentions=decoder_outputs.attentions,\n","            cross_attentions=decoder_outputs.cross_attentions,\n","            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n","            encoder_hidden_states=encoder_outputs.hidden_states,\n","            encoder_attentions=encoder_outputs.attentions,\n","        )"]},{"cell_type":"markdown","source":["##Load Model##"],"metadata":{"id":"MjnmrOlyyUi8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"riYFX5jfBd5I"},"outputs":[],"source":["model_untrained_rat = T5ForMultimodalGeneration.from_pretrained(args.model, patch_size=patch_size, padding_idx=padding_idx, save_dir=save_dir) \n","model_pretrained_rat = T5ForMultimodalGeneration.from_pretrained('/content/cs182final_proj/mm-cot/experiments/rationale_allenai-unifiedqa-t5-small_detr_QCM-LE_lr5e-05_bs32_op512_ep20/', patch_size=patch_size, padding_idx=padding_idx, save_dir=save_dir) \n","\n","print(\"model parameters: \", model_untrained_rat.num_parameters())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K5j_iaExBSdX"},"outputs":[],"source":["# only use the last model for evaluation to save time\n","datacollator = DataCollatorForSeq2Seq(tokenizer)\n","if args.final_eval:\n","    training_args = Seq2SeqTrainingArguments(\n","        save_dir,\n","        do_train=True if args.evaluate_dir is None else False,\n","        do_eval=False,\n","        evaluation_strategy=\"no\",\n","        logging_strategy=\"steps\",\n","        save_strategy=\"epoch\",\n","        save_total_limit = 0,\n","        learning_rate= args.lr,\n","        eval_accumulation_steps=args.eval_acc,\n","        per_device_train_batch_size=args.bs,\n","        per_device_eval_batch_size=args.eval_bs,\n","        weight_decay=0.01,\n","        num_train_epochs=args.epoch,\n","        predict_with_generate=args.use_generate,\n","        report_to=\"none\",\n","    )\n","# evaluate at each epoch\n","else:\n","    training_args = Seq2SeqTrainingArguments(\n","        save_dir,\n","        do_train=True if args.evaluate_dir is None else False,\n","        do_eval=True,\n","        evaluation_strategy=\"epoch\",\n","        logging_strategy=\"steps\",\n","        save_strategy=\"epoch\",\n","        save_total_limit = 0,\n","        learning_rate= args.lr,\n","        eval_accumulation_steps=args.eval_acc,\n","        per_device_train_batch_size=args.bs,\n","        per_device_eval_batch_size=args.eval_bs,\n","        weight_decay=0.01,\n","        num_train_epochs=args.epoch,\n","        metric_for_best_model=\"accuracy\" if args.prompt_format != \"QCM-LE\" else \"rougeL\",\n","        predict_with_generate=args.use_generate,\n","        load_best_model_at_end=True,\n","        report_to=\"none\",\n","    )\n","\n","example_trainer_rationale = Seq2SeqTrainer(\n","    model=model_untrained_rat,\n","    args=training_args,\n","    train_dataset=train_set,\n","    eval_dataset=eval_set,\n","    data_collator=datacollator,\n","    tokenizer=tokenizer,\n","    compute_metrics = compute_metrics_acc if args.prompt_format != \"QCM-LE\" else compute_metrics_rougel\n",")\n","\n","actual_trainer_rationale = Seq2SeqTrainer(\n","    model=model_pretrained_rat,\n","    args=training_args,\n","    train_dataset=train_set,\n","    eval_dataset=eval_set,\n","    data_collator=datacollator,\n","    tokenizer=tokenizer,\n","    compute_metrics = compute_metrics_acc if args.prompt_format != \"QCM-LE\" else compute_metrics_rougel\n",")"]},{"cell_type":"markdown","metadata":{"id":"HW3lhxrks2Y3"},"source":["# (e) Visualization of rationales on examples\n","\n","In the following blocks, the model has not been trained yet. However, we will show what the model currently predicts with the random intialized weights (rationale generation model with no training). "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1LRP_Yb1PVF5"},"outputs":[],"source":["# Choose whatever question ids you want to use\n","example_qids = ['2'];"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ygjpx4qTEfRK"},"outputs":[],"source":["example_set = ScienceQADatasetImg(\n","    problems,\n","    example_qids,\n","    name_maps,\n","    tokenizer,\n","    args.input_len,\n","    args.output_len,\n","    args,\n","    image_features,\n","    args.test_le,\n",")\n","predict_results = example_trainer_rationale.predict(test_dataset=example_set, max_length=args.output_len)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wb0oHsZNQuMB"},"outputs":[],"source":["def decode_pred(pred):\n","  if args.use_generate:\n","      preds, targets = pred.predictions, pred.label_ids\n","  else:\n","      preds = pred.predictions[0]\n","      targets = pred.label_ids\n","      preds = preds.argmax(axis=2)\n","\n","  preds = tokenizer.batch_decode(\n","      preds, skip_special_tokens=True, clean_up_tokenization_spaces=True\n","  )\n","  targets = tokenizer.batch_decode(\n","      targets, skip_special_tokens=True, clean_up_tokenization_spaces=True\n","  )\n","  return preds, targets\n","\n","preds, targets = decode_pred(predict_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mFR-QLUCjo1U"},"outputs":[],"source":["f\"Untrained Prediction: {preds[0]}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vQzMzuuqqZex"},"outputs":[],"source":["targets[0]"]},{"cell_type":"markdown","metadata":{"id":"Cj1zcZavzCjh"},"source":["i. What pattern do you see in the rationales generated from the model that hasn’t been trained yet? Why do you think this pattern exists? \n"]},{"cell_type":"markdown","metadata":{"id":"JyJ1n8EeUcSF"},"source":["# Now we will actually train the model. Run the following training cells. This model is loaded with pretrained weights and you will finish running the last two epochs of the model. This should take about 5 minutes.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v7_0DwPwTyAA"},"outputs":[],"source":["#@title training\n","if args.evaluate_dir is None:\n","    actual_trainer_rationale.train()\n","    actual_trainer_rationale.save_model(save_dir)\n","metrics = actual_trainer_rationale.evaluate(eval_dataset = test_set)\n","actual_trainer_rationale.log_metrics(\"test\", metrics)\n","predict_results = actual_trainer_rationale.predict(test_dataset=test_set, max_length=args.output_len)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A1eJEL2ZtSkZ"},"outputs":[],"source":["actual_result = actual_trainer_rationale.predict(test_dataset=example_set, max_length=args.output_len)"]},{"cell_type":"markdown","metadata":{"id":"VmqFR-eKorJd"},"source":["Trained Rational Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VAvKhYRtoqOd"},"outputs":[],"source":["actual_pred, actual_target = decode_pred(actual_result)\n","actual_pred[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cDm3Cz8Jow7O"},"outputs":[],"source":["actual_target[0]"]},{"cell_type":"markdown","metadata":{"id":"s0OCcTM7t5HQ"},"source":["#Answer Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"KPgKQBV6uPoL"},"outputs":[],"source":["#@title Set arguments for answer inference\n","args = parse_args()\n","args.model  = \"allenai/unifiedqa-t5-base\"\n","args.user_msg = \"answer\"\n","args.img_type = \"detr\"\n","args.bs = 1\n","args.eval_bs = 1\n","args.eval_acc = 10\n","args.output_len = 64\n","args.final_eval = True\n","args.prompt_format = \"QCMG-A\"\n","# Change this\n","args.eval_le = \"predictions_ans_eval.json\"\n","args.test_le = \"predictions_ans_test.json\"\n","args_answer = copy.deepcopy(args)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"FaeeGDAZvuK4"},"outputs":[],"source":["#@title Load data and image features for answer inference\n","# training logger to log training progress\n","training_logger = Table(\n","    Column(\"Epoch\", justify=\"center\"),\n","    Column(\"Steps\", justify=\"center\"),\n","    Column(\"Loss\", justify=\"center\"),\n","    title=\"Training Status\",\n","    pad_edge=False,\n","    box=box.ASCII,\n",")\n","\n","#args = parse_args()\n","print(\"args\",args)\n","print('====Input Arguments====')\n","print(json.dumps(vars(args), indent=2, sort_keys=False))\n","\n","random.seed(args.seed)\n","\n","if not os.path.exists(args.output_dir):\n","        os.mkdir(args.output_dir)\n","\n","if args.img_type is not None:\n","    problems, qids, name_maps, image_features = load_data_img(args)  # problems, test question ids, shot example ids\n","    dataframe = {'problems':problems, 'qids':qids, 'name_maps': name_maps, 'image_features': image_features}\n","else:\n","    problems, qids = load_data_std(args)  # problems, test question ids, shot example ids\n","    dataframe = {'problems':problems, 'qids':qids}"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"AvtCduz-xPkT"},"outputs":[],"source":["#@title Split qids for answer inference\n","torch.manual_seed(args.seed)  # pytorch random seed\n","np.random.seed(args.seed)  # numpy random seed\n","torch.backends.cudnn.deterministic = True\n","\n","if args.evaluate_dir is not None:\n","    args.model = args.evaluate_dir\n","\n","tokenizer = T5Tokenizer.from_pretrained(args.model)\n","\n","console.log(f\"\"\"[Model]: Loading {args.model}...\\n\"\"\")\n","console.log(f\"[Data]: Reading data...\\n\")\n","problems = dataframe['problems']\n","qids = dataframe['qids']\n","train_qids = qids['train']\n","test_qids = qids['test']\n","val_qids = qids['val']\n","\n","if args.evaluate_dir is not None:\n","    save_dir = args.evaluate_dir\n","else:\n","    model_name = args.model.replace(\"/\",\"-\")\n","    gpu_count = torch.cuda.device_count()\n","    save_dir = f\"{args.output_dir}/{args.user_msg}_{model_name}_{args.img_type}_{args.prompt_format}_lr{args.lr}_bs{args.bs * gpu_count}_op{args.output_len}_ep{args.epoch}\"\n","    if not os.path.exists(save_dir):\n","        os.mkdir(save_dir)\n","\n","padding_idx = tokenizer._convert_token_to_id(tokenizer.pad_token)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RUpLfIa9xfgf"},"outputs":[],"source":["#@title Create dataloader for answer inference\n","patch_size = img_shape[args.img_type]\n","#model = T5ForMultimodalGeneration.from_pretrained(args.model, patch_size=patch_size, padding_idx=padding_idx, save_dir=save_dir)\n","name_maps = dataframe['name_maps']\n","image_features = dataframe['image_features']\n","train_set = ScienceQADatasetImg(\n","    problems,\n","    train_qids,\n","    name_maps,\n","    tokenizer,\n","    args.input_len,\n","    args.output_len,\n","    args,\n","    image_features,\n",")\n","example_set = ScienceQADatasetImg(\n","    problems,\n","    example_qids,\n","    name_maps,\n","    tokenizer,\n","    args.input_len,\n","    args.output_len,\n","    args,\n","    image_features,\n","    args.test_le,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AaYQwsJ1y_lt"},"outputs":[],"source":["model_ans = T5ForMultimodalGeneration.from_pretrained(args.model, patch_size=patch_size, padding_idx=padding_idx, save_dir=save_dir) \n","pretrain_model_ans = T5ForMultimodalGeneration.from_pretrained('/content/cs182final_proj/mm-cot/experiments/answer_allenai-unifiedqa-t5-small_detr_QCMG-A_lr5e-05_bs32_op64_ep20', patch_size=patch_size, padding_idx=padding_idx, save_dir=save_dir) \n","print(\"model parameters: \", model_ans.num_parameters())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YfisCmSkzBmR"},"outputs":[],"source":["#@title Prepare trainer for answer inference\n","# only use the last model for evaluation to save time\n","datacollator = DataCollatorForSeq2Seq(tokenizer)\n","if args.final_eval:\n","    training_args = Seq2SeqTrainingArguments(\n","        save_dir,\n","        do_train=True if args.evaluate_dir is None else False,\n","        do_eval=False,\n","        evaluation_strategy=\"no\",\n","        logging_strategy=\"steps\",\n","        save_strategy=\"epoch\",\n","        save_total_limit = 0,\n","        learning_rate= args.lr,\n","        eval_accumulation_steps=args.eval_acc,\n","        per_device_train_batch_size=args.bs,\n","        per_device_eval_batch_size=args.eval_bs,\n","        weight_decay=0.01,\n","        num_train_epochs=args.epoch,\n","        predict_with_generate=args.use_generate,\n","        report_to=\"none\",\n","    )\n","# evaluate at each epoch\n","else:\n","    training_args = Seq2SeqTrainingArguments(\n","        save_dir,\n","        do_train=True if args.evaluate_dir is None else False,\n","        do_eval=True,\n","        evaluation_strategy=\"epoch\",\n","        logging_strategy=\"steps\",\n","        save_strategy=\"epoch\",\n","        save_total_limit = 0,\n","        learning_rate= args.lr,\n","        eval_accumulation_steps=args.eval_acc,\n","        per_device_train_batch_size=args.bs,\n","        per_device_eval_batch_size=args.eval_bs,\n","        weight_decay=0.01,\n","        num_train_epochs=args.epoch,\n","        metric_for_best_model=\"accuracy\" if args.prompt_format != \"QCM-LE\" else \"rougeL\",\n","        predict_with_generate=args.use_generate,\n","        load_best_model_at_end=True,\n","        report_to=\"none\",\n","    )\n","\n","trainer_ans = Seq2SeqTrainer(\n","    model=model_ans,\n","    args=training_args,\n","    train_dataset=train_set,\n","    eval_dataset=eval_set,\n","    data_collator=datacollator,\n","    tokenizer=tokenizer,\n","    compute_metrics = compute_metrics_acc if args.prompt_format != \"QCM-LE\" else compute_metrics_rougel\n",")\n","\n","pretrain_trainer_ans = Seq2SeqTrainer(\n","    model=pretrain_model_ans,\n","    args=training_args,\n","    train_dataset=train_set,\n","    eval_dataset=eval_set,\n","    data_collator=datacollator,\n","    tokenizer=tokenizer,\n","    compute_metrics = compute_metrics_acc if args.prompt_format != \"QCM-LE\" else compute_metrics_rougel\n",")"]},{"cell_type":"markdown","metadata":{"id":"wAvVorUn0RnJ"},"source":["**Answer Inference (inference only)**\n","\n","In the following blocks, the model has not been trained yet. However, we will show what the model currently predicts with the random intialized weights (0 training answer inference). "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RxQ4j0PMzRtE"},"outputs":[],"source":["predict_results = trainer_ans.predict(test_dataset=example_set, max_length=args.output_len)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kTOCh5gMzard"},"outputs":[],"source":["preds, targets = decode_pred(predict_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g4cp-N3D0DlG"},"outputs":[],"source":["preds[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4nE7trbCz15H"},"outputs":[],"source":["targets[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vdFRQU5V1DeL"},"outputs":[],"source":["args.epoch = 1"]},{"cell_type":"markdown","metadata":{"id":"KhKD5FXPVxNn"},"source":["# Now the following cells will actually train the second model (answer inference)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SpU_rQOKEO0T"},"outputs":[],"source":["# @title Training \n","if args.evaluate_dir is None:\n","    pretrain_trainer_ans.train()\n","    pretrain_trainer_ans.save_model(save_dir)\n","    \n","metrics = pretrain_trainer_ans.evaluate(eval_dataset = test_set)\n","pretrain_trainer_ans.log_metrics(\"test\", metrics)\n","pretrain_trainer_ans.save_metrics(\"test\", metrics)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3btYwbOip1id"},"outputs":[],"source":["actual_result = pretrain_trainer_ans.predict(test_dataset=example_set, max_length=args.output_len)\n","\n","preds, targets = decode_pred(actual_result)\n","\n","preds[0]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_HG_NfYfp1EN"},"outputs":[],"source":["targets[0]"]},{"cell_type":"markdown","metadata":{"id":"-efQhm60r-Tt"},"source":["ii. With your now fully trained model, run inference on some data points! Do the rationales make sense? Do they help answer the question? Compare them to the rationales generated before the training.\n","\n","Play around with the model by modifying the example_set to run inference on the different values in the dataset. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0E4jIEx0p16M"},"outputs":[],"source":["example_qids = ['4']\n","example_set_rationale = ScienceQADatasetImg(\n","    problems,\n","    example_qids,\n","    name_maps,\n","    tokenizer,\n","    args_rationale.input_len,\n","    args_rationale.output_len,\n","    args_rationale,\n","    image_features,\n","    args_rationale.test_le,\n",")\n","\n","example_set_answer = ScienceQADatasetImg(\n","    problems,\n","    example_qids,\n","    name_maps,\n","    tokenizer,\n","    args_answer.input_len,\n","    args_answer.output_len,\n","    args_answer,\n","    image_features,\n","    args_answer.test_le,\n",")\n","\n","rationale = actual_trainer_rationale.predict(test_dataset=example_set_rationale, max_length=args_rationale.output_len)\n","actual_result = pretrain_trainer_ans.predict(test_dataset=example_set_answer, max_length=args_answer.output_len)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uMQmPy_mOTPs"},"outputs":[],"source":["example_set_rationale.data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MPGSSwF2sgQv"},"outputs":[],"source":["preds_inference, targets_inference = decode_pred(actual_result)\n","preds_rationale, targets_rationale = decode_pred(rationale)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nz1FAlXfKVbS"},"outputs":[],"source":["targets_rationale[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h3m2zViLtr22"},"outputs":[],"source":["preds_rationale[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qhVVRbTHgiHk"},"outputs":[],"source":["preds_inference[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRb5uar4gmhh"},"outputs":[],"source":["targets_inference[0]"]},{"cell_type":"markdown","metadata":{"id":"AnwLmczyEE-z"},"source":["# (f) Image robustness experimentation and visualization\n","\n","Congratulations! You (hopefully) have a working multimodal CoT model! In this part of the assignment, you will explore the robustness of the multimodality of this model and see what happens when you feed incorrect data in. \n","\n","i. Run part (f) of the notebook. In this part, you will experiment on a single example to better visualize and understand how the model you trained may or may not be robust to incorrect inputs. This first block simply displays the example discussed beforehand (the bottom feeder question), along with the text inputs and the image inputs. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4WepGuARRbsG"},"outputs":[],"source":["import copy\n","\n","our_name_maps = copy.deepcopy(name_maps)\n","our_name_maps['7'] = '32'\n","\n","wrong_qids = ['4']\n","wrong_set_rationale = ScienceQADatasetImg(\n","    problems,\n","    ['7'],\n","    our_name_maps,\n","    tokenizer,\n","    args_rationale.input_len,\n","    args_rationale.output_len,\n","    args_rationale,\n","    image_features,\n","    args_rationale.test_le,\n",")\n","\n","wrong_set_answer = ScienceQADatasetImg(\n","    problems,\n","    ['7'],\n","    our_name_maps,\n","    tokenizer,\n","    args_answer.input_len,\n","    args_answer.output_len,\n","    args_answer,\n","    image_features,\n","    args_answer.test_le,\n",")\n","\n","rationale_wrong = actual_trainer_rationale.predict(test_dataset=wrong_set_rationale, max_length=args_rationale.output_len)\n","actual_result_wrong = pretrain_trainer_ans.predict(test_dataset=wrong_set_answer, max_length=args_answer.output_len)\n"]},{"cell_type":"markdown","source":["### The following blocks of code will take a sample question that has an associated image attached to it, swap that image with another question's image, and then try running that question (in other words, the image attached into the model will be entirely unrelated to the question being asked by the prompt). "],"metadata":{"id":"-XJ66ZoNWxbH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VEpBQr1kkWic"},"outputs":[],"source":["preds_inference_wrong, targets_inference_wrong = decode_pred(actual_result_wrong)\n","preds_rationale_wrong, targets_rationale_wrong, = decode_pred(rationale_wrong)"]},{"cell_type":"code","source":["targets_rationale_wrong[0]"],"metadata":{"id":"AdCxqmbzm4d2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The correct image to our problem\n","index = 7\n","sampleProb = (problems[str(index)])\n","print(get_question_text(sampleProb) + \"\\n\")\n","print()\n","path = '/content/cs182final_proj/mm-cot/image_vals/' + str(index) + '_'\n","\n","if os.path.isfile(path + 'choice_0.png'):\n","  display(Image(path + 'choice_0.png'))\n","print(\"\\n\")\n","if os.path.isfile(path + 'choice_1.png'):\n","  display(Image(path + 'choice_1.png'))"],"metadata":{"id":"Fig62YtZz-F9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The wrong image that we're putting in\n","index = 60\n","sampleProb = (problems[str(index)])\n","print(get_question_text(sampleProb) + \"\\n\")\n","print()\n","path = '/content/cs182final_proj/mm-cot/image_vals/' + str(index) + '_'\n","\n","if os.path.isfile(path + 'choice_0.png'):\n","  display(Image(path + 'choice_0.png'))\n","print(\"\\n\")\n","if os.path.isfile(path + 'choice_1.png'):\n","  display(Image(path + 'choice_1.png'))"],"metadata":{"id":"BleTKlcE0AuK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Td0aiUfNBea"},"outputs":[],"source":["targets_inference_wrong[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YGFk-Zkekbo4"},"outputs":[],"source":["preds_rationale_wrong[0]"]},{"cell_type":"code","source":["preds_inference_wrong[0]"],"metadata":{"id":"7JCu2zPCm-sd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title This is the original prompt and target answer for the bottom feeder question. Is it the same as the cells run above?\n","\n","index = 7\n","\n","shot_qids = ['1', '10']\n","test_qid = str(index)\n","\n","input, target = build_train_pair(problems, test_qid, args)\n","input"],"metadata":{"id":"hEnO0SVbY_25"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target"],"metadata":{"id":"D7NOcAuWZH-L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# What do you notice between the original inference (where the question has the correct image) versus the \"wrong\" inference (where the question has an incorrect image)? What does this suggest about this model's reliance on the image modality for performance? Write your answer in the text box below. "],"metadata":{"id":"Hv3cpWIiXdmM"}},{"cell_type":"markdown","source":["Write your answer here"],"metadata":{"id":"np0vbrioX2rY"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"19FdymmEqGXbvtW8A5mAN_iPx_IU1VvRm","timestamp":1683266416475},{"file_id":"1cPpsxAaOIk5ZsvQQfE7wPPq2ZeWXY9uu","timestamp":1682112189980}],"gpuType":"T4"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
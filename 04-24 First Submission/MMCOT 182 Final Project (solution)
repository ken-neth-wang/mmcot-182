{"cells":[{"cell_type":"markdown","metadata":{"id":"WihAr34xnxSz"},"source":["# Multi-modal Chain of Thought Model for ScienceQA problems "]},{"cell_type":"markdown","source":["Chain of thought (CoT) refers to the mental process of reasoning and inference that humans use to arrive at an answer or solution to a problem. It involves synthesizing information from multiple sources, making logical connections between ideas, and integrating them into a coherent line of reasoning. In the context of natural language processing and deep learning models, CoT prompting involves generating intermediate reasoning steps to arrive at the final answer to a question. This process allows the model to break down complex questions into simpler, more manageable steps, and to leverage information from multiple sources, such as text and images, to arrive at a more accurate answer. This homework assignment is heavily adapted from this paper.\n","\n","This homework assignment will use the Science Question Answering (ScienceQA) dataset, comprises a total of 21,000+ multiple-choice science questions sourced from elementary and high school curricula. Through this notebook, you’ll explore a subset of this dataset, which consists of questions that only contain an a text context as well as questions that have both text and image contexts. \n","\n","This homework assignment will walk you through the sequential steps towards building a multimodal chain-of-thought model that utilizes both text and image inputs and chain-of-thought reasoning to solve ScienceQA problems. \n"],"metadata":{"id":"4tShjtOFuJh1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rXwhMaUjb4_K","cellView":"form"},"outputs":[],"source":["#@title Mount your Google Drive\n","\n","import os\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wi1iKX-ssky3","cellView":"form"},"outputs":[],"source":["#@title Set up mount symlink\n","\n","DRIVE_PATH = '/content/gdrive/My\\ Drive/cs182final_proj'\n","DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n","if not os.path.exists(DRIVE_PYTHON_PATH):\n","  %mkdir $DRIVE_PATH\n","\n","## the space in `My Drive` causes some issues,\n","## make a symlink to avoid this\n","SYM_PATH = '/content/cs182final_proj'\n","if not os.path.exists(SYM_PATH):\n","  !ln -s $DRIVE_PATH $SYM_PATH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_PQTBDkB9tSo","cellView":"form"},"outputs":[],"source":["#@title Clone homework repo\n","\n","%cd $SYM_PATH\n","\n","######## TODO #########\n","if not os.path.exists(\"mm-cot\"):\n","  !git clone https://github.com/kevinjcai/mm-cot.git\n","######## TODO #########\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JEakJINJ8xIg","cellView":"form"},"outputs":[],"source":["#@title Install dependencies\n","!pip install huggingface-hub>=0.4.0\n","!pip install numpy==1.23.2\n","!pip install openai==0.23.0\n","!pip install pandas==1.4.3\n","!pip install rouge==1.0.1\n","!pip install sentence-transformers==2.2.2\n","!pip install transformers==4.21.1\n","!pip install nltk==3.6.6\n","!pip install evaluate==0.4.0\n","!pip install rouge==1.0.1\n","!pip install rouge_score==0.1.2\n","!pip install rich>=13.3.2\n","!pip install -r req.txt\n","!pip install -U numpy\n","!pip install -U pandas\n","!pip install gdown\n"]},{"cell_type":"markdown","source":["**Restart Runtime**\n","You will likely need to restart runtime after install dependencies. Do it! "],"metadata":{"id":"XAvR4FwFm1YZ"}},{"cell_type":"code","source":["#@title importing ntlk\n","import nltk\n","\n","# It will error the first time you run this cell\n","nltk.download('punkt')"],"metadata":{"id":"jyz1EVW6o7Q7","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title download punkt \n","nltk.download('punkt')"],"metadata":{"id":"pLwFYacFLR7E","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title import gdown\n","import gdown\n","%cd mm-cot\n","!pwd"],"metadata":{"id":"2W4DdqZmMzYM","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8LwXgJox_LOf","cellView":"form"},"outputs":[],"source":["#@title Download pretrained model weights\n","\n","# If gdown fails or errors, go to the drive links directly and manually import the necessary files.\n","url = \"https://drive.google.com/drive/u/0/folders/1bHOG__E9AKMNeJ5P81gdSEWNpNIQXRqu\"\n","\n","gdown.download_folder(url, quiet=False, use_cookies=False)\n"]},{"cell_type":"code","source":["#@title Download images for dataset\n","# If gdown fails or errors, go to the drive links directly and manually import the necessary files.\n","\n","url = 'https://drive.google.com/drive/folders/1x7oPd31TPhWJowy3D4aGW2OsS6GVa2D5'\n","gdown.download_folder(url, quiet=False)"],"metadata":{"id":"rjTyHi2VSLvN","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7EMRaU8MYbcu","cellView":"form"},"outputs":[],"source":[" #@title Download vision features\n","\n","# If gdown fails or errors, go to the drive links directly and manually import the necessary files.\n","\n","# if not os.path.exists(\"vision_features\"):\n","%cd mm-cot\n","url = \"https://drive.google.com/drive/u/0/folders/10_DDp59tJwJafg6Kykg82GlZqQj_Irmz\"\n","gdown.download_folder(url, quiet=False, use_cookies=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IL1guy0YtXan","cellView":"form"},"outputs":[],"source":["#@title Download data \n","# cloning scienceqa dataset \n","#if not os.path.exists(\"data\"):\n","%cd mm-cot\n","!git clone https://github.com/lupantech/ScienceQA.git\n","!mv ScienceQA/data ./\n","!rm -r ScienceQA\n","# gdown.download_folder(url, quiet=False, use_cookies=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Evq0jF99GHah"},"outputs":[],"source":["import sys\n","import os\n","import numpy as np\n","import torch\n","import os\n","import re\n","import json\n","import argparse\n","import random\n","from IPython.display import Image, display\n","from transformers import T5Tokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, T5ForConditionalGeneration\n","from utils_data import img_shape, load_data_std, load_data_img\n","from utils_prompt import *\n","from utils_evaluate import get_scores\n","from rich.table import Column, Table\n","from rich import box\n","from rich.console import Console\n","console = Console(record=True)\n","from torch import cuda\n","import nltk\n","import evaluate\n","\n","sys.path.append('/content/gdrive/MyDrive/182_mmcot')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AA1Xz2Nea21X","cellView":"form"},"outputs":[],"source":["#@title Initialize args\n","# This creates the config arguments for the next part.\n","#import argparse\n","#argparse.ArgumentParser()\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('-f')\n","    parser.add_argument('--data_root', type=str, default='data')\n","    parser.add_argument('--output_dir', type=str, default='experiments')\n","    parser.add_argument('--model', type=str, default='allenai/unifiedqa-t5-base')\n","    parser.add_argument('--options', type=list, default=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n","    parser.add_argument('--epoch', type=int, default=1)\n","    parser.add_argument('--lr', type=float, default=5e-5)\n","    parser.add_argument('--bs', type=int, default=1)\n","    parser.add_argument('--input_len', type=int, default=512)\n","    parser.add_argument('--output_len', type=int, default=64)\n","    parser.add_argument('--eval_bs', type=int, default=1)\n","    parser.add_argument('--eval_acc', type=int, default=None, help='evaluate accumulation step')\n","    parser.add_argument('--train_split', type=str, default='minitrain', choices=['train', 'trainval', 'minitrain'])\n","    parser.add_argument('--val_split', type=str, default='minival', choices=['test', 'val', 'minival'])\n","    parser.add_argument('--test_split', type=str, default='minitest', choices=['test', 'minitest'])\n","    \n","    parser.add_argument('--use_generate', action='store_true', help='only for baseline to improve inference speed')\n","    parser.add_argument('--final_eval', action='store_true', help='only evaluate the model at the final epoch')\n","    parser.add_argument('--user_msg', type=str, default=\"baseline\", help='experiment type in the save_dir')\n","    parser.add_argument('--img_type', type=str, default=None, choices=['detr', 'clip', 'resnet'], help='type of image features')\n","    parser.add_argument('--eval_le', type=str, default=None, help='generated rationale for the dev set')\n","    parser.add_argument('--test_le', type=str, default=None, help='generated rationale for the test set')\n","    parser.add_argument('--evaluate_dir', type=str, default=None, help='the directory of model for evaluation')\n","    parser.add_argument('--caption_file', type=str, default='data/captions.json')\n","    parser.add_argument('--use_caption', action='store_true', help='use image captions or not')\n","    parser.add_argument('--prompt_format', type=str, default='QCM-A', help='prompt format template',\n","                        choices=['QCM-A', 'QCM-LE', 'QCMG-A', 'QCM-LEA', 'QCM-ALE'])\n","    parser.add_argument('--seed', type=int, default=42, help='random seed')\n","\n","    args = parser.parse_args()\n","    return args\n","args = parse_args()"]},{"cell_type":"markdown","metadata":{"id":"9OTn179rtUAz"},"source":["# a) Prompt Building\n","\n","Prompt building is commonly used in tasks such as text completion, translation, and question answering, where the model is required to generate output that is consistent with a given input prompt. Prompt building can be a highly effective technique for improving the accuracy and performance of language models, as it allows us to fine-tune the model's behavior for specific tasks and domains. Additionally, prompt building can help to mitigate the problem of bias in language models, as it provides a way to explicitly specify the desired output and constrain the model's behavior.\n"]},{"cell_type":"markdown","source":["# This block will show exactly one problem and the associated text data. \n"],"metadata":{"id":"SztFNXQSvhiX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QdZePNHwo0Jb"},"outputs":[],"source":["# This is the raw json file that contains data. \n","# This block will show exactly one problem and the associated text data. \n","problems = json.load(open('/content/cs182final_proj/mm-cot/data/scienceqa/problems.json'))\n","problems['7']"]},{"cell_type":"markdown","metadata":{"id":"Vybzl_cbpG42"},"source":["The following block prints out the question text as well as the context. In the raw json file, the `hint` corresponds to the question context."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U0r3v27Koowx"},"outputs":[],"source":["index = 7 ### FEEL FREE TO TRY CHANGING THE INDEX TO GET DIFFERENT QUESTIONS from the dataset\n","\n","sampleProb = (problems[str(index)])\n","print(get_question_text(sampleProb))\n","print(\"Context: \" + get_context_text(sampleProb, False))\n","print(get_choice_text(sampleProb, args.options))\n","print(get_lecture_text(sampleProb))\n"]},{"cell_type":"markdown","source":["# a) i. \n","\n","This chain-of-thought model uses a two-stage framework. The first stage generates the rationale, which is trained with the \"solution\" text as the target. Run the block to see what the \"solution\" corresponds to for the question you saw above. How might this solution help the second model to get the answer?\n"],"metadata":{"id":"PsdUpR26pyO6"}},{"cell_type":"code","source":["get_solution_text(sampleProb)"],"metadata":{"id":"iwq0BYpCpjKj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# a) ii. \n","What are the components of the input prompt to the first stage? In other words, which pieces of a specific datapoint in the ScienceQA dataset do you concatenate together to generate the input prompt? Write this out and maintain the order that each component is attached. \n","hint: Look at build_train_pair and the following blocks below. \n"],"metadata":{"id":"A7cNRsO1v1AV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VoAt1sZ0fLQ1"},"outputs":[],"source":["index = 7\n","\n","shot_qids = ['1', '10']\n","test_qid = str(index)\n","\n","input, target = build_train_pair(problems, test_qid, args)"]},{"cell_type":"code","source":["input"],"metadata":{"id":"kI0FKzppZdig"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target"],"metadata":{"id":"jT2M_d99Ze_r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sE-ZbwRzniJQ"},"source":["# a) ii. Conceptual Question: `build_train_pair` creates the prompt that represents the exact text input into the model. Write out the exact components of this prompt in the right order (ie. the raw dataset consists of different components such as the question text, the choices text, etc. How does `build_train_pair` actually build the prompt that serves as the input?)"]},{"cell_type":"markdown","source":["Feel free to write your answer here."],"metadata":{"id":"IWtxvZe8qsVr"}},{"cell_type":"markdown","metadata":{"id":"oyHAl4wWp5bM"},"source":["# b) Add images\n"]},{"cell_type":"markdown","metadata":{"id":"4uBeZQ-onVAG"},"source":["Note that not all questions are associated with an image - the ScienceQA dataset comprises 10,332 (48.7%) questions with an image context, 10,220 (48.2%) with a text context, and 6,532 (30.8%) with both modalities.\n"]},{"cell_type":"markdown","source":["# b) i. \n","Run the following cells You can try other indices and see the images as well as the questions they correspond to. To save space, this notebook only downloads certain images (question indices that produce an image are 7, 28, 45, 60). Notice that some indices produce only one image corresponding to the overall question, while other questions also produce images that correspond to the answers. Does adding the picture(s) make the question easier to solve? How might this inform our model?\n"],"metadata":{"id":"ZuH-SN2cwJ_t"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZgiavNknK-9"},"outputs":[],"source":["index = 7\n","path = '/content/gdrive/MyDrive/182_mmcot/image_vals/' + str(index) + '_'\n","if os.path.isfile(path + 'choice_0.png'):\n","  display(Image(path + 'choice_0.png'))\n","if os.path.isfile(path + 'choice_1.png'):\n","  display(Image(path + 'choice_1.png'))\n","if os.path.isfile(path + 'image.png'):\n","  display(Image(path + 'image.png'))"]},{"cell_type":"code","source":["sampleProb = (problems[str(index)])\n","print(get_question_text(sampleProb))"],"metadata":{"id":"I5psy5O-pF3E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For context, here is the answer that you should have gotten. \n","# Is it easier to get to this answer now that you've seen theimage?\n","print(get_origin_answer(sampleProb, args.options))\n","print(get_answer(sampleProb, args.options))\n"],"metadata":{"id":"NxsTeh3zpC9G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KF4yTN4-qSji"},"source":["# (c) Dataloading\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5K5__4DfuRtD","cellView":"form"},"outputs":[],"source":["#@title Set arguments (for rationale generation later)\n","args.model  = \"allenai/unifiedqa-t5-small\"\n","args.user_msg = \"rationale\"\n","args.img_type = \"detr\"\n","args.bs = 1\n","args.eval_bs = 1\n","args.eval_acc = 10\n","args.output_len = 512\n","args.final_eval = True\n","args.prompt_format = \"QCM-LE\""]},{"cell_type":"code","source":["from torch.utils.data import Dataset"],"metadata":{"id":"E8kBftTstQKV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i0Dr7ZBk9Mk8","cellView":"form"},"outputs":[],"source":["#@title set dataset for std (no image)\n","import os\n","import json\n","import numpy as np\n","import torch\n","from utils_prompt import *\n","\n","img_shape = {\n","    \"resnet\": (512, 2048),\n","    \"clip\": (49, 2048),\n","    \"detr\": (100, 256),\n","}\n","\n","def load_data_std(args):\n","    problems = json.load(open(os.path.join(args.data_root, 'scienceqa/problems.json')))\n","    pid_splits = json.load(open(os.path.join(args.data_root, 'scienceqa/pid_splits.json')))\n","    captions = json.load(open(args.caption_file))[\"captions\"]\n","\n","    for qid in problems:\n","        problems[qid]['caption'] = captions[qid] if qid in captions else \"\"\n","\n","    train_qids = pid_splits['%s' % (args.train_split)]\n","    val_qids = pid_splits['%s' % (args.val_split)]\n","    test_qids = pid_splits['%s' % (args.test_split)]\n","    print(f\"number of train problems: {len(train_qids)}\\n\")\n","    print(f\"number of val problems: {len(val_qids)}\\n\")\n","    print(f\"number of test problems: {len(test_qids)}\\n\")\n","\n","    qids = {'train': train_qids, 'val':val_qids,'test':test_qids}\n","    return problems, qids,\n","\n","def load_data_img(args):\n","    problems = json.load(open(os.path.join(args.data_root, 'scienceqa/problems.json')))\n","    pid_splits = json.load(open(os.path.join(args.data_root, 'scienceqa/pid_splits.json')))\n","    captions = json.load(open(args.caption_file))[\"captions\"]\n","    name_maps = json.load(open('vision_features/name_map.json'))\n","\n","    # check\n","    if args.img_type == \"resnet\":\n","        image_features = np.load('vision_features/resnet.npy')\n","        image_features = np.expand_dims(image_features, axis=1)\n","        image_features = image_features.repeat(512, axis=1)\n","    elif args.img_type == \"clip\":\n","        image_features = np.load('vision_features/clip.npy')\n","    elif args.img_type == \"detr\":\n","        image_features = np.load('vision_features/detr.npy')\n","    else:\n","        image_features = np.load('vision_features/detr.npy')\n","    print(\"img_features size: \", image_features.shape)\n","\n","    for qid in problems:\n","        problems[qid]['caption'] = captions[qid] if qid in captions else \"\"\n","\n","    train_qids = pid_splits['%s' % (args.train_split)]\n","    val_qids = pid_splits['%s' % (args.val_split)]\n","    test_qids = pid_splits['%s' % (args.test_split)]\n","    print(f\"number of train problems: {len(train_qids)}\\n\")\n","    print(f\"number of val problems: {len(val_qids)}\\n\")\n","    print(f\"number of test problems: {len(test_qids)}\\n\")\n","\n","    qids = {'train': train_qids, 'val':val_qids,'test':test_qids}\n","    return problems, qids, name_maps, image_features\n","\n","class ScienceQADatasetStd(Dataset):\n","    \"\"\"\n","    Creating a custom dataset for reading the dataset and\n","    loading it into the dataloader to pass it to the\n","    neural network for finetuning the model\n","\n","    \"\"\"\n","\n","    def __init__(\n","        self, problems, qids, tokenizer, source_len, target_len, args, test_le=None\n","    ):\n","        self.tokenizer = tokenizer\n","        self.data = {qid : problems[qid] for qid in qids}\n","        self.source_len = source_len\n","        self.summ_len = target_len\n","        self.target_text = []\n","        self.source_text = []\n","        if test_le is not None:\n","            test_le_data =json.load(open(test_le))[\"preds\"]\n","        else:\n","            test_le_data = None\n","        idx = 0\n","        for qid in self.data:\n","            if test_le_data is not None:\n","                curr_le_data = test_le_data[idx]\n","                idx += 1\n","            else:\n","                curr_le_data = None\n","            prompt, target = build_train_pair(problems, qid, args, curr_le_data)\n","            self.target_text.append(target)\n","            self.source_text.append(prompt)\n","\n","    def __len__(self):\n","        return len(self.target_text)\n","\n","    def __getitem__(self, index):\n","        source_text = str(self.source_text[index])\n","        target_text = str(self.target_text[index])\n","\n","        # cleaning data so as to ensure data is in string type\n","        source_text = \" \".join(source_text.split())\n","        target_text = \" \".join(target_text.split())\n","\n","        source = self.tokenizer.batch_encode_plus(\n","            [source_text],\n","            max_length=self.source_len,\n","            pad_to_max_length=True,\n","            truncation=True,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\",\n","        )\n","        target = self.tokenizer.batch_encode_plus(\n","            [target_text],\n","            max_length=self.summ_len,\n","            pad_to_max_length=True,\n","            truncation=True,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\",\n","        )\n","        source_ids = source[\"input_ids\"].squeeze()\n","        source_mask = source[\"attention_mask\"].squeeze()\n","        target_ids = target[\"input_ids\"].squeeze().tolist()\n","        \n","        return {\n","            \"input_ids\": source_ids,\n","            \"attention_mask\": source_mask,\n","            \"labels\": target_ids,\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IKXMtM_o2677","cellView":"form"},"outputs":[],"source":["#@title rerun main\n","# training logger to log training progress\n","training_logger = Table(\n","    Column(\"Epoch\", justify=\"center\"),\n","    Column(\"Steps\", justify=\"center\"),\n","    Column(\"Loss\", justify=\"center\"),\n","    title=\"Training Status\",\n","    pad_edge=False,\n","    box=box.ASCII,\n",")\n","\n","#args = parse_args()\n","print(\"args\",args)\n","print('====Input Arguments====')\n","print(json.dumps(vars(args), indent=2, sort_keys=False))\n","\n","random.seed(args.seed)\n","\n","if not os.path.exists(args.output_dir):\n","        os.mkdir(args.output_dir)\n","\n","if args.img_type is not None:\n","    problems, qids, name_maps, image_features = load_data_img(args)  # problems, test question ids, shot example ids\n","    dataframe = {'problems':problems, 'qids':qids, 'name_maps': name_maps, 'image_features': image_features}\n","else:\n","    problems, qids = load_data_std(args)  # problems, test question ids, shot example ids\n","    dataframe = {'problems':problems, 'qids':qids}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yeWoeE0orhC8","cellView":"form"},"outputs":[],"source":["#@title rerun before creating dataloader\n","torch.manual_seed(args.seed)  # pytorch random seed\n","np.random.seed(args.seed)  # numpy random seed\n","torch.backends.cudnn.deterministic = True\n","\n","if args.evaluate_dir is not None:\n","    args.model = args.evaluate_dir\n","\n","tokenizer = T5Tokenizer.from_pretrained(args.model)\n","\n","console.log(f\"\"\"[Model]: Loading {args.model}...\\n\"\"\")\n","console.log(f\"[Data]: Reading data...\\n\")\n","problems = dataframe['problems']\n","qids = dataframe['qids']\n","train_qids = qids['train']\n","test_qids = qids['test']\n","val_qids = qids['val']\n","\n","if args.evaluate_dir is not None:\n","    save_dir = args.evaluate_dir\n","else:\n","    model_name = args.model.replace(\"/\",\"-\")\n","    gpu_count = torch.cuda.device_count()\n","    save_dir = f\"{args.output_dir}/{args.user_msg}_{model_name}_{args.img_type}_{args.prompt_format}_lr{args.lr}_bs{args.bs * gpu_count}_op{args.output_len}_ep{args.epoch}\"\n","    if not os.path.exists(save_dir):\n","        os.mkdir(save_dir)\n","\n","padding_idx = tokenizer._convert_token_to_id(tokenizer.pad_token)"]},{"cell_type":"markdown","source":["# c) Dataloading \n","### CODING PART BELOW \n","Dataloading typically involves reading data from one or more sources, such as a file or a database, and performing preprocessing steps such as normalization, transformation, or augmentation, in order to prepare the data for use in the model.This process determines how efficiently and effectively the model can learn from the data. \n"],"metadata":{"id":"niQQPNfBxCSJ"}},{"cell_type":"markdown","source":["i. Implement part (c) in the notebook. Initialize and tokenize the prompt and the target. Run the “Dataloader Test” code cell to check your answer.\n","Hint: to initialize the prompt and the target, use the given prompt building utils.\n"],"metadata":{"id":"QepZXFYcxSCx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3BCScfkjxX72"},"outputs":[],"source":["class ScienceQADatasetImg(Dataset):\n","    \"\"\"\n","    Creating a custom dataset for reading the dataset and\n","    loading it into the dataloader to pass it to the\n","    neural network for finetuning the model\n","    \"\"\"\n","\n","    def __init__(\n","        self, problems, qids, name_maps, tokenizer, source_len, target_len, args, image_features, test_le=None\n","    ):\n","        \"\"\"\n","        Initializes a Dataset class\n","        Args:\n","            dataframe (pandas.DataFrame): Input dataframe\n","            tokenizer (transformers.tokenizer): Transformers tokenizer\n","            source_len (int): Max length of source text\n","            target_len (int): Max length of target text\n","            source_text (str): column name of source text\n","            target_text (str): column name of target text\n","        \"\"\"\n","        self.tokenizer = tokenizer\n","        self.data = {qid : problems[qid] for qid in qids}\n","        self.source_len = source_len\n","        self.summ_len = target_len\n","        self.target_text = []\n","        self.source_text = []\n","        self.image_ids = []\n","        if test_le is not None:\n","            test_le_data =json.load(open(test_le))[\"preds\"]\n","        else:\n","            test_le_data = None\n","        idx = 0\n","        for qid in self.data:\n","            if test_le_data is not None:\n","                curr_le_data = test_le_data[idx]\n","                idx += 1\n","            else:\n","                curr_le_data = None\n","\n","\n","            ### TO DO ###\n","\n","            ### HINT: Look at utils_prompt.py ###\n","            prompt, target = build_train_pair(problems, qid, args, curr_le_data)\n","            self.target_text.append(target)\n","            self.source_text.append(prompt)\n","\n","            #############\n","            if str(qid) in name_maps:\n","                i_vectors = image_features[int(name_maps[str(qid)])]\n","                self.image_ids.append(i_vectors)\n","            else:\n","                shape = img_shape[args.img_type]\n","                self.image_ids.append(np.zeros(shape))\n","    \n","    def __len__(self):\n","        \"\"\"returns the length of dataframe\"\"\"\n","\n","        return len(self.target_text)\n","\n","    def __getitem__(self, index):\n","        \"\"\"return the input ids, attention masks and target ids\"\"\"\n","\n","        source_text = str(self.source_text[index])\n","        target_text = str(self.target_text[index])\n","        image_ids = self.image_ids[index]\n","\n","        # cleaning data so as to ensure data is in string type\n","        source_text = \" \".join(source_text.split())\n","        target_text = \" \".join(target_text.split())\n","\n","        ### TO DO: Implement tokenizer. Documentation here: \n","        ###https://huggingface.co/transformers/v2.11.0/main_classes/tokenizer.html \n","        ### HINT: You'll want to tokenize batches \n","        source = self.tokenizer.batch_encode_plus(\n","            [source_text],\n","            max_length=self.source_len,\n","            pad_to_max_length=True,\n","            truncation=True,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\",\n","        )\n","        target = self.tokenizer.batch_encode_plus(\n","            [target_text],\n","            max_length=self.summ_len,\n","            pad_to_max_length=True,\n","            truncation=True,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\",\n","        )\n","        #############\n","        source_ids = source[\"input_ids\"].squeeze()\n","        source_mask = source[\"attention_mask\"].squeeze()\n","        target_ids = target[\"input_ids\"].squeeze().tolist()\n","\n","        image_ids = torch.tensor(image_ids).squeeze()\n","        \n","        return {\n","            \"input_ids\": source_ids,\n","            \"attention_mask\": source_mask,\n","            \"image_ids\": image_ids,\n","            \"labels\": target_ids,\n","        }"]},{"cell_type":"markdown","metadata":{"id":"izAG2k_JrGe-"},"source":["# Test whether your data loader works"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8McqrsqHq8Cv","cellView":"form"},"outputs":[],"source":["#@title rerun for creating dataloader\n","patch_size = img_shape[args.img_type]\n","#model = T5ForMultimodalGeneration.from_pretrained(args.model, patch_size=patch_size, padding_idx=padding_idx, save_dir=save_dir)\n","name_maps = dataframe['name_maps']\n","image_features = dataframe['image_features']\n","train_set = ScienceQADatasetImg(\n","    problems,\n","    train_qids,\n","    name_maps,\n","    tokenizer,\n","    args.input_len,\n","    args.output_len,\n","    args,\n","    image_features,\n",")\n","eval_set = ScienceQADatasetImg(\n","    problems,\n","    val_qids,\n","    name_maps,\n","    tokenizer,\n","    args.input_len,\n","    args.output_len,\n","    args,\n","    image_features,\n","    args.eval_le,\n",")\n","test_set = ScienceQADatasetImg(\n","    problems,\n","    test_qids,\n","    name_maps,\n","    tokenizer,\n","    args.input_len,\n","    args.output_len,\n","    args,\n","    image_features,\n","    args.test_le,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uDUitkUw-OUm"},"outputs":[],"source":["#@title Test Dataloader\n","# test train set\n","expected_input_ids = np.array([11860,    10,  4073,    13,   175,  2315,    19,   623,   189,   222,\n","         3414,    58,  1193,  6327,    10,   445,    87,   188, 17011,    10,\n","           41,   188,    61,  1013,  5089,    41,   279,    61, 17903,  2834,\n","           41,   254,    61, 10585,    41,   308,    61, 13782, 17942,    10,\n","            1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0])\n","assert np.allclose(np.array(train_set[7]['input_ids']), expected_input_ids)\n"]},{"cell_type":"markdown","metadata":{"id":"UP4EUiPDta_L"},"source":["# d) Model architecture "]},{"cell_type":"markdown","source":["i. The two-stage framework takes the input (text and image), generates rationale, and then appends the rationale to the original input to create a modified input. This modified input is then passed into the second model, the inference model. What architectural structure (covered in the course) is this reminiscent of or analogous to?"],"metadata":{"id":"2czXLv4wydJy"}},{"cell_type":"markdown","source":["ii. Refer to the Homework PDF for information on the model architecture. "],"metadata":{"id":"VlYD9sQPxXAA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5_jzn5MrA4U9","cellView":"form"},"outputs":[],"source":["#@title Metrics (do not need to do anything here)\n","def extract_ans(ans):\n","    pattern = re.compile(r'The answer is \\(([A-Z])\\)')\n","    res = pattern.findall(ans)\n","    \n","    if len(res) == 1:\n","        answer = res[0]  # 'A', 'B', ...\n","    else:\n","        answer = \"FAILED\" \n","    return answer  \n","\n","# accuracy for answer inference\n","def compute_metrics_acc(eval_preds):\n","    if args.use_generate:\n","        preds, targets = eval_preds\n","        if isinstance(preds, tuple):\n","            preds = preds[0]\n","    else:\n","        preds = eval_preds.predictions[0]\n","        targets = eval_preds.label_ids\n","        preds = preds.argmax(axis=2)\n","    preds = tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","    targets = tokenizer.batch_decode(targets, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","    correct = 0\n","    assert len(preds) == len(targets)\n","    for idx, pred in enumerate(preds):\n","        reference = targets[idx]\n","        reference = extract_ans(reference)\n","        extract_pred = extract_ans(pred)\n","        best_option = extract_pred\n","        if reference == best_option:\n","            correct +=1 \n","    return {'accuracy': 1.0*correct/len(targets)}\n","\n","# rougel for rationale generation\n","metric = evaluate.load(\"rouge\")\n","def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [label.strip() for label in labels]\n","    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n","    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n","    return preds, labels\n","\n","def compute_metrics_rougel(eval_preds):\n","    if args.use_generate:\n","        preds, targets = eval_preds\n","        if isinstance(preds, tuple):\n","            preds = preds[0]\n","    else:\n","        preds = eval_preds.predictions[0]\n","        targets = eval_preds.label_ids\n","        preds = preds.argmax(axis=2)\n","    preds = tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","    targets = tokenizer.batch_decode(targets, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","\n","    decoded_preds, decoded_labels = postprocess_text(preds, targets)\n","\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    result = {k: round(v * 100, 4) for k, v in result.items()}\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    return result"]},{"cell_type":"markdown","metadata":{"id":"yJgovrvHBjUB"},"source":["# Model building here \n","### CODING PART HERE (implement the TODO sections). Refer to the PDF. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rT7z_vbuBiBg"},"outputs":[],"source":["#@title model\n","from transformers import T5Config, T5ForConditionalGeneration\n","from transformers.models.t5.modeling_t5 import T5Stack, __HEAD_MASK_WARNING_MSG, T5EncoderModel\n","import copy\n","import math\n","import os\n","import warnings\n","from typing import Optional, Tuple, Union\n","import torch\n","from torch import nn\n","from torch.nn import CrossEntropyLoss\n","from transformers.modeling_outputs import (\n","    BaseModelOutput,\n","    Seq2SeqLMOutput,\n",")\n","\n","class T5ForMultimodalGeneration(T5ForConditionalGeneration):\n","    _keys_to_ignore_on_load_missing = [\n","        r\"encoder.embed_tokens.weight\",\n","        r\"decoder.embed_tokens.weight\",\n","        r\"lm_head.weight\",\n","    ]\n","    _keys_to_ignore_on_load_unexpected = [\n","        r\"decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight\",\n","    ]\n","\n","    def __init__(self, config: T5Config, patch_size, padding_idx, save_dir):\n","        super().__init__(config)\n","        self.model_dim = config.d_model\n","        \n","        self.padding_idx = padding_idx\n","        self.out = open(os.path.join(save_dir, 'gate.txt'), 'w')\n","\n","\n","        ## TODO IMPLEMENT MODEL HERE \n","        # HINT START WITH EMBEDDING\n","        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n","        self.patch_num, self.patch_dim = patch_size\n","\n","        self.image_dense = nn.Linear(self.patch_dim, config.d_model)\n","        self.mha_layer = torch.nn.MultiheadAttention(embed_dim=config.hidden_size, kdim=config.hidden_size, vdim=config.hidden_size, num_heads=1, batch_first=True)\n","        self.gate_dense = nn.Linear(2*config.hidden_size, config.hidden_size)\n","        self.sigmoid = nn.Sigmoid()\n","        ########################\n","\n","        ## comment the encoder and decoder \n","        encoder_config = copy.deepcopy(config)\n","        encoder_config.is_decoder = False\n","        encoder_config.use_cache = False\n","        encoder_config.is_encoder_decoder = False\n","\n","        ## TODO: Impelement encoder. Hint:  use t5stack\n","        self.encoder = T5Stack(encoder_config, self.shared)\n","        ####################################\n","\n","        decoder_config = copy.deepcopy(config)\n","        decoder_config.is_decoder = True\n","        decoder_config.is_encoder_decoder = False\n","        decoder_config.num_layers = config.num_decoder_layers\n","        ################################\n","\n","        ## TODO: Impelement decoder. Hint:  use t5stack\n","        self.decoder = T5Stack(decoder_config, self.shared)\n","        ############################\n","\n","        ## TODO: implement the lm_head layer\n","        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n","\n","        ######################################################\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","        # Model parallel\n","        self.model_parallel = False\n","        self.device_map = None\n","\n","    def forward(\n","        self,\n","        input_ids: Optional[torch.LongTensor] = None,\n","        image_ids=None,\n","        attention_mask: Optional[torch.FloatTensor] = None,\n","        decoder_input_ids: Optional[torch.LongTensor] = None,\n","        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n","        head_mask: Optional[torch.FloatTensor] = None,\n","        decoder_head_mask: Optional[torch.FloatTensor] = None,\n","        cross_attn_head_mask: Optional[torch.Tensor] = None,\n","        encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n","        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n","        inputs_embeds: Optional[torch.FloatTensor] = None,\n","        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n","        labels: Optional[torch.LongTensor] = None,\n","        use_cache: Optional[bool] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n","        use_cache = use_cache if use_cache is not None else self.config.use_cache\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n","        if head_mask is not None and decoder_head_mask is None:\n","            if self.config.num_layers == self.config.num_decoder_layers:\n","                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)\n","                decoder_head_mask = head_mask\n","\n","        # Encode if needed (training, first prediction pass)\n","        if encoder_outputs is None:\n","            # Convert encoder inputs in embeddings if needed\n","            encoder_outputs = self.encoder(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                inputs_embeds=inputs_embeds,\n","                head_mask=head_mask,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","            )\n","\n","        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n","            encoder_outputs = BaseModelOutput(\n","                last_hidden_state=encoder_outputs[0],\n","                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n","                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n","            )\n","\n","        hidden_states = encoder_outputs[0]\n","        \n","\n","        ## TODO IMPLEMENT THE FORWARD FUNCTION\n","        image_embedding = self.image_dense(image_ids)\n","        image_att, _ = self.mha_layer(hidden_states, image_embedding, image_embedding)\n","\n","        merge = torch.cat([hidden_states, image_att], dim=-1)\n","        gate = self.sigmoid(self.gate_dense(merge))\n","        hidden_states = (1 - gate) * hidden_states + gate * image_att\n","        ############################################\n","\n","        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n","            # get decoder inputs from shifting lm labels to the right\n","            decoder_input_ids = self._shift_right(labels)\n","\n","        \n","        # Decode\n","        decoder_outputs = self.decoder(\n","            input_ids=decoder_input_ids,\n","            attention_mask=decoder_attention_mask,\n","            inputs_embeds=decoder_inputs_embeds,\n","            past_key_values=past_key_values,\n","            encoder_hidden_states=hidden_states,\n","            encoder_attention_mask=attention_mask,\n","            head_mask=decoder_head_mask,\n","            cross_attn_head_mask=cross_attn_head_mask,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = decoder_outputs[0]\n","\n","        if self.config.tie_word_embeddings:\n","            # Rescale output before projecting on vocab\n","            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n","            sequence_output = sequence_output * (self.model_dim**-0.5)\n","\n","        lm_logits = self.lm_head(sequence_output)\n","\n","        loss = None\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss(ignore_index=-100)\n","            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n","\n","        if not return_dict:\n","            output = (lm_logits,) + decoder_outputs[1:] + encoder_outputs\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return Seq2SeqLMOutput(\n","            loss=loss,\n","            logits=lm_logits,\n","            past_key_values=decoder_outputs.past_key_values,\n","            decoder_hidden_states=decoder_outputs.hidden_states,\n","            decoder_attentions=decoder_outputs.attentions,\n","            cross_attentions=decoder_outputs.cross_attentions,\n","            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n","            encoder_hidden_states=encoder_outputs.hidden_states,\n","            encoder_attentions=encoder_outputs.attentions,\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"riYFX5jfBd5I"},"outputs":[],"source":["model = T5ForMultimodalGeneration.from_pretrained(args.model, patch_size=patch_size, padding_idx=padding_idx, save_dir=save_dir) \n","model_pretrained_ep20 = T5ForMultimodalGeneration.from_pretrained('/content/cs182final_proj/mm-cot/experiments/rationale_allenai-unifiedqa-t5-small_detr_QCM-LE_lr5e-05_bs32_op512_ep20/', patch_size=patch_size, padding_idx=padding_idx, save_dir=save_dir) \n","\n","print(\"model parameters: \", model.num_parameters())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K5j_iaExBSdX"},"outputs":[],"source":["#@title rerun prepare trainer\n","# only use the last model for evaluation to save time\n","datacollator = DataCollatorForSeq2Seq(tokenizer)\n","if args.final_eval:\n","    training_args = Seq2SeqTrainingArguments(\n","        save_dir,\n","        do_train=True if args.evaluate_dir is None else False,\n","        do_eval=False,\n","        evaluation_strategy=\"no\",\n","        logging_strategy=\"steps\",\n","        save_strategy=\"epoch\",\n","        save_total_limit = 0,\n","        learning_rate= args.lr,\n","        eval_accumulation_steps=args.eval_acc,\n","        per_device_train_batch_size=args.bs,\n","        per_device_eval_batch_size=args.eval_bs,\n","        weight_decay=0.01,\n","        num_train_epochs=args.epoch,\n","        predict_with_generate=args.use_generate,\n","        report_to=\"none\",\n","    )\n","# evaluate at each epoch\n","else:\n","    training_args = Seq2SeqTrainingArguments(\n","        save_dir,\n","        do_train=True if args.evaluate_dir is None else False,\n","        do_eval=True,\n","        evaluation_strategy=\"epoch\",\n","        logging_strategy=\"steps\",\n","        save_strategy=\"epoch\",\n","        save_total_limit = 0,\n","        learning_rate= args.lr,\n","        eval_accumulation_steps=args.eval_acc,\n","        per_device_train_batch_size=args.bs,\n","        per_device_eval_batch_size=args.eval_bs,\n","        weight_decay=0.01,\n","        num_train_epochs=args.epoch,\n","        metric_for_best_model=\"accuracy\" if args.prompt_format != \"QCM-LE\" else \"rougeL\",\n","        predict_with_generate=args.use_generate,\n","        load_best_model_at_end=True,\n","        report_to=\"none\",\n","    )\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_set,\n","    eval_dataset=eval_set,\n","    data_collator=datacollator,\n","    tokenizer=tokenizer,\n","    compute_metrics = compute_metrics_acc if args.prompt_format != \"QCM-LE\" else compute_metrics_rougel\n",")\n","\n","actual_trainer_rationale = Seq2SeqTrainer(\n","    model=model_pretrained_ep20,\n","    args=training_args,\n","    train_dataset=train_set,\n","    eval_dataset=eval_set,\n","    data_collator=datacollator,\n","    tokenizer=tokenizer,\n","    compute_metrics = compute_metrics_acc if args.prompt_format != \"QCM-LE\" else compute_metrics_rougel\n",")"]},{"cell_type":"markdown","source":["# (e) Visualization of rationales on examples\n","\n","In the following blocks, the model has not been trained yet. However, we will show what the model currently predicts with the random intialized weights (rationale generation model with no training). "],"metadata":{"id":"HW3lhxrks2Y3"}},{"cell_type":"code","source":["# Choose whatever question ids you want to use\n","example_qids = ['2'];"],"metadata":{"id":"1LRP_Yb1PVF5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ygjpx4qTEfRK"},"outputs":[],"source":["example_set = ScienceQADatasetImg(\n","    problems,\n","    example_qids,\n","    name_maps,\n","    tokenizer,\n","    args.input_len,\n","    args.output_len,\n","    args,\n","    image_features,\n","    args.test_le,\n",")\n","predict_results = trainer.predict(test_dataset=example_set, max_length=args.output_len)\n"]},{"cell_type":"code","source":["def decode_pred(pred):\n","  if args.use_generate:\n","      preds, targets = pred.predictions, pred.label_ids\n","  else:\n","      preds = pred.predictions[0]\n","      targets = pred.label_ids\n","      preds = preds.argmax(axis=2)\n","\n","  preds = tokenizer.batch_decode(\n","      preds, skip_special_tokens=True, clean_up_tokenization_spaces=True\n","  )\n","  targets = tokenizer.batch_decode(\n","      targets, skip_special_tokens=True, clean_up_tokenization_spaces=True\n","  )\n","  return preds, targets\n","\n","preds, targets = decode_pred(predict_results)\n"],"metadata":{"id":"wb0oHsZNQuMB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f\"Untrained Prediction {preds[0]}\"\n"],"metadata":{"id":"mFR-QLUCjo1U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["targets[0]"],"metadata":{"id":"vQzMzuuqqZex"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["i. What pattern do you see in the rationales generated from the model that hasn’t been trained yet? Why do you think this pattern exists? \n"],"metadata":{"id":"Cj1zcZavzCjh"}},{"cell_type":"markdown","source":["# Now we will actually train the model. Run the following training cells. This model is loaded with pretrained weights and you will finish running the last two epochs of the model. This should take about 10 minutes.\n"],"metadata":{"id":"JyJ1n8EeUcSF"}},{"cell_type":"code","source":["#@title training \n","if args.evaluate_dir is None:\n","    actual_trainer.train()\n","  \n","    actual_trainer.save_model(save_dir)\n","    \n","metrics = actual_trainer.evaluate(eval_dataset = test_set)\n","actual_trainer.log_metrics(\"test\", metrics)\n","actual_trainer.save_metrics(\"test\", metrics)\n","\n","predict_results = actual_trainer.predict(test_dataset=test_set, max_length=args.output_len) \n","if actual_trainer.is_world_process_zero():\n","    if args.use_generate:\n","        preds, targets = predict_results.predictions, predict_results.label_ids\n","    else:\n","        preds = predict_results.predictions[0]\n","        targets = predict_results.label_ids\n","        preds = preds.argmax(axis=2)\n","\n","    preds = tokenizer.batch_decode(\n","        preds, skip_special_tokens=True, clean_up_tokenization_spaces=True\n","    )\n","    targets = tokenizer.batch_decode(\n","        targets, skip_special_tokens=True, clean_up_tokenization_spaces=True\n","    )\n","\n","    results_ans = {}\n","    results_rationale = {}\n","    results_reference = {}\n","    \n","    num_fail = 0\n","    for idx, qid in enumerate(test_qids):\n","        pred = preds[int(idx)]\n","        ref = targets[int(idx)]\n","        extract_pred = extract_ans(pred)\n","        if extract_pred != \"FAILED\":\n","            if extract_pred in args.options:\n","                extract_pred = args.options.index(extract_pred)\n","            else:\n","                extract_pred = random.choice(range(0,len(args.options)))\n","        else:\n","            num_fail += 1\n","            extract_pred = random.choice(range(len(args.options))) # random choose one option\n","        results_ans[str(qid)] = extract_pred\n","        results_rationale[str(qid)] = pred\n","        results_reference[str(qid)] = ref\n","\n","    scores = get_scores(results_ans, results_rationale, results_reference, os.path.join(args.data_root, \"scienceqa/problems.json\"))\n","    preds = [pred.strip() for pred in preds]\n","    output_data = {\n","            \"num_fail\": num_fail,\n","            \"scores\": scores,\n","            \"preds\": preds,\n","              \"labels\": targets}\n","    output_prediction_file = os.path.join(save_dir,\"predictions_ans_test.json\")\n","    with open(output_prediction_file, \"w\") as writer:\n","        writer.write(json.dumps(output_data, indent=4))\n","\n","# generate the rationale for the eval set\n","if args.prompt_format == \"QCM-LE\":\n","    torch.cuda.empty_cache()\n","    del predict_results, preds, targets\n","    predict_results = actual_trainer.predict(test_dataset=eval_set, max_length=args.output_len) \n","    if actual_trainer.is_world_process_zero():\n","        if args.use_generate:\n","            preds, targets = predict_results.predictions, predict_results.label_ids\n","        else:\n","            preds = predict_results.predictions[0]\n","            targets = predict_results.label_ids\n","            preds = preds.argmax(axis=2)\n","\n","        preds = tokenizer.batch_decode(\n","            preds, skip_special_tokens=True, clean_up_tokenization_spaces=True\n","        )\n","        targets = tokenizer.batch_decode(\n","            targets, skip_special_tokens=True, clean_up_tokenization_spaces=True\n","        )\n","        preds = [pred.strip() for pred in preds]\n","        output_data = {\"preds\": preds,\n","              \"labels\": targets}\n","        output_prediction_file = os.path.join(save_dir,\"predictions_ans_eval.json\")\n","        with open(output_prediction_file, \"w\") as writer:\n","            writer.write(json.dumps(output_data, indent=4))"],"metadata":{"id":"v7_0DwPwTyAA","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["actual_result = actual_trainer_rationale.predict(test_dataset=example_set, max_length=args.output_len)"],"metadata":{"id":"A1eJEL2ZtSkZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Trained Rational Inference"],"metadata":{"id":"VmqFR-eKorJd"}},{"cell_type":"code","source":["actual_pred, actual_target = decode_pred(actual_result)\n","acutal_pred[0]\n"],"metadata":{"id":"VAvKhYRtoqOd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["actual_target[0]"],"metadata":{"id":"cDm3Cz8Jow7O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Answer Inference"],"metadata":{"id":"s0OCcTM7t5HQ"}},{"cell_type":"code","source":["#@title Set arguments for answer inference\n","args = parse_args()\n","args.model  = \"allenai/unifiedqa-t5-base\"\n","args.user_msg = \"answer\"\n","args.img_type = \"detr\"\n","args.bs = 1\n","args.eval_bs = 1\n","args.eval_acc = 10\n","args.output_len = 64\n","args.final_eval = True\n","args.prompt_format = \"QCMG-A\"\n","# Change this\n","args.eval_le = \"/content/cs182final_proj/mm-cot/experiments/rationale_allenai-unifiedqa-t5-small_detr_QCM-LE_lr5e-05_bs32_op512_ep20/predictions_ans_eval.json\"\n","args.test_le = \"/content/cs182final_proj/mm-cot/experiments/rationale_allenai-unifiedqa-t5-small_detr_QCM-LE_lr5e-05_bs32_op512_ep20/predictions_ans_test.json\""],"metadata":{"id":"KPgKQBV6uPoL","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title rerun main\n","# training logger to log training progress\n","training_logger = Table(\n","    Column(\"Epoch\", justify=\"center\"),\n","    Column(\"Steps\", justify=\"center\"),\n","    Column(\"Loss\", justify=\"center\"),\n","    title=\"Training Status\",\n","    pad_edge=False,\n","    box=box.ASCII,\n",")\n","\n","#args = parse_args()\n","print(\"args\",args)\n","print('====Input Arguments====')\n","print(json.dumps(vars(args), indent=2, sort_keys=False))\n","\n","random.seed(args.seed)\n","\n","if not os.path.exists(args.output_dir):\n","        os.mkdir(args.output_dir)\n","\n","if args.img_type is not None:\n","    problems, qids, name_maps, image_features = load_data_img(args)  # problems, test question ids, shot example ids\n","    dataframe = {'problems':problems, 'qids':qids, 'name_maps': name_maps, 'image_features': image_features}\n","else:\n","    problems, qids = load_data_std(args)  # problems, test question ids, shot example ids\n","    dataframe = {'problems':problems, 'qids':qids}"],"metadata":{"id":"FaeeGDAZvuK4","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title rerun before creating dataloader\n","torch.manual_seed(args.seed)  # pytorch random seed\n","np.random.seed(args.seed)  # numpy random seed\n","torch.backends.cudnn.deterministic = True\n","\n","if args.evaluate_dir is not None:\n","    args.model = args.evaluate_dir\n","\n","tokenizer = T5Tokenizer.from_pretrained(args.model)\n","\n","console.log(f\"\"\"[Model]: Loading {args.model}...\\n\"\"\")\n","console.log(f\"[Data]: Reading data...\\n\")\n","problems = dataframe['problems']\n","qids = dataframe['qids']\n","train_qids = qids['train']\n","test_qids = qids['test']\n","val_qids = qids['val']\n","\n","if args.evaluate_dir is not None:\n","    save_dir = args.evaluate_dir\n","else:\n","    model_name = args.model.replace(\"/\",\"-\")\n","    gpu_count = torch.cuda.device_count()\n","    save_dir = f\"{args.output_dir}/{args.user_msg}_{model_name}_{args.img_type}_{args.prompt_format}_lr{args.lr}_bs{args.bs * gpu_count}_op{args.output_len}_ep{args.epoch}\"\n","    if not os.path.exists(save_dir):\n","        os.mkdir(save_dir)\n","\n","padding_idx = tokenizer._convert_token_to_id(tokenizer.pad_token)"],"metadata":{"id":"AvtCduz-xPkT","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title rerun for creating dataloader\n","patch_size = img_shape[args.img_type]\n","#model = T5ForMultimodalGeneration.from_pretrained(args.model, patch_size=patch_size, padding_idx=padding_idx, save_dir=save_dir)\n","name_maps = dataframe['name_maps']\n","image_features = dataframe['image_features']\n","train_set = ScienceQADatasetImg(\n","    problems,\n","    train_qids,\n","    name_maps,\n","    tokenizer,\n","    args.input_len,\n","    args.output_len,\n","    args,\n","    image_features,\n",")\n","example_set = ScienceQADatasetImg(\n","    problems,\n","    example_qids,\n","    name_maps,\n","    tokenizer,\n","    args.input_len,\n","    args.output_len,\n","    args,\n","    image_features,\n","    args.test_le,\n",")\n","#     name_maps,\n","#     tokenizer,\n","#     args.input_len,\n","#     args.output_len,\n","#     args,\n","#     image_features,\n","#     args.test_le,\n","# )"],"metadata":{"id":"RUpLfIa9xfgf","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = T5ForMultimodalGeneration.from_pretrained(args.model, patch_size=patch_size, padding_idx=padding_idx, save_dir=save_dir) \n","pretrain_model = T5ForMultimodalGeneration.from_pretrained('/content/cs182final_proj/mm-cot/experiments/answer_allenai-unifiedqa-t5-small_detr_QCMG-A_lr5e-05_bs32_op64_ep20', patch_size=patch_size, padding_idx=padding_idx, save_dir=save_dir) \n","print(\"model parameters: \", model.num_parameters())"],"metadata":{"id":"AaYQwsJ1y_lt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title rerun prepare trainer\n","# only use the last model for evaluation to save time\n","datacollator = DataCollatorForSeq2Seq(tokenizer)\n","if args.final_eval:\n","    training_args = Seq2SeqTrainingArguments(\n","        save_dir,\n","        do_train=True if args.evaluate_dir is None else False,\n","        do_eval=False,\n","        evaluation_strategy=\"no\",\n","        logging_strategy=\"steps\",\n","        save_strategy=\"epoch\",\n","        save_total_limit = 0,\n","        learning_rate= args.lr,\n","        eval_accumulation_steps=args.eval_acc,\n","        per_device_train_batch_size=args.bs,\n","        per_device_eval_batch_size=args.eval_bs,\n","        weight_decay=0.01,\n","        num_train_epochs=args.epoch,\n","        predict_with_generate=args.use_generate,\n","        report_to=\"none\",\n","    )\n","# evaluate at each epoch\n","else:\n","    training_args = Seq2SeqTrainingArguments(\n","        save_dir,\n","        do_train=True if args.evaluate_dir is None else False,\n","        do_eval=True,\n","        evaluation_strategy=\"epoch\",\n","        logging_strategy=\"steps\",\n","        save_strategy=\"epoch\",\n","        save_total_limit = 0,\n","        learning_rate= args.lr,\n","        eval_accumulation_steps=args.eval_acc,\n","        per_device_train_batch_size=args.bs,\n","        per_device_eval_batch_size=args.eval_bs,\n","        weight_decay=0.01,\n","        num_train_epochs=args.epoch,\n","        metric_for_best_model=\"accuracy\" if args.prompt_format != \"QCM-LE\" else \"rougeL\",\n","        predict_with_generate=args.use_generate,\n","        load_best_model_at_end=True,\n","        report_to=\"none\",\n","    )\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_set,\n","    eval_dataset=eval_set,\n","    data_collator=datacollator,\n","    tokenizer=tokenizer,\n","    compute_metrics = compute_metrics_acc if args.prompt_format != \"QCM-LE\" else compute_metrics_rougel\n",")\n","\n","pretrain_trainer = Seq2SeqTrainer(\n","    model=pretrain_model,\n","    args=training_args,\n","    train_dataset=train_set,\n","    eval_dataset=eval_set,\n","    data_collator=datacollator,\n","    tokenizer=tokenizer,\n","    compute_metrics = compute_metrics_acc if args.prompt_format != \"QCM-LE\" else compute_metrics_rougel\n",")"],"metadata":{"id":"YfisCmSkzBmR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Answer Inference (inference only)\n","In the following blocks, the model has not been trained yet. However, we will show what the model currently predicts with the random intialized weights (0 training answer inference). "],"metadata":{"id":"wAvVorUn0RnJ"}},{"cell_type":"code","source":["predict_results = trainer.predict(test_dataset=example_set, max_length=args.output_len)\n"],"metadata":{"id":"RxQ4j0PMzRtE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# if args.use_generate:\n","#     preds, targets = predict_results.predictions, predict_results.label_ids\n","# else:\n","#     preds = predict_results.predictions[0]\n","#     targets = predict_results.label_ids\n","#     preds = preds.argmax(axis=2)\n","\n","# preds = tokenizer.batch_decode(\n","#     preds, skip_special_tokens=True, clean_up_tokenization_spaces=True\n","# )\n","# targets = tokenizer.batch_decode(\n","#     targets, skip_special_tokens=True, clean_up_tokenization_spaces=True\n","# )\n","# TODO DELETE above\n","preds, targets = decode_pred(predict_results)\n"],"metadata":{"id":"kTOCh5gMzard"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preds[0]"],"metadata":{"id":"g4cp-N3D0DlG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["targets[0]"],"metadata":{"id":"4nE7trbCz15H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args.epoch = 1"],"metadata":{"id":"vdFRQU5V1DeL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Now the following cells will actually train the second model (answer inference)"],"metadata":{"id":"KhKD5FXPVxNn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SpU_rQOKEO0T","cellView":"form"},"outputs":[],"source":["#@title training \n","if args.evaluate_dir is None:\n","    trainer.train()\n","    # trainer.save_model(save_dir)\n","    \n","metrics = trainer.evaluate(eval_dataset = test_set)\n","trainer.log_metrics(\"test\", metrics)\n","trainer.save_metrics(\"test\", metrics)\n","\n","predict_results = trainer.predict(test_dataset=test_set, max_length=args.output_len) \n","if trainer.is_world_process_zero():\n","    if args.use_generate:\n","        preds, targets = predict_results.predictions, predict_results.label_ids\n","    else:\n","        preds = predict_results.predictions[0]\n","        targets = predict_results.label_ids\n","        preds = preds.argmax(axis=2)\n","\n","    preds = tokenizer.batch_decode(\n","        preds, skip_special_tokens=True, clean_up_tokenization_spaces=True\n","    )\n","    targets = tokenizer.batch_decode(\n","        targets, skip_special_tokens=True, clean_up_tokenization_spaces=True\n","    )\n","\n","    results_ans = {}\n","    results_rationale = {}\n","    results_reference = {}\n","    \n","    num_fail = 0\n","    for idx, qid in enumerate(test_qids):\n","        pred = preds[int(idx)]\n","        ref = targets[int(idx)]\n","        extract_pred = extract_ans(pred)\n","        if extract_pred != \"FAILED\":\n","            if extract_pred in args.options:\n","                extract_pred = args.options.index(extract_pred)\n","            else:\n","                extract_pred = random.choice(range(0,len(args.options)))\n","        else:\n","            num_fail += 1\n","            extract_pred = random.choice(range(len(args.options))) # random choose one option\n","        results_ans[str(qid)] = extract_pred\n","        results_rationale[str(qid)] = pred\n","        results_reference[str(qid)] = ref\n","\n","    scores = get_scores(results_ans, results_rationale, results_reference, os.path.join(args.data_root, \"scienceqa/problems.json\"))\n","    preds = [pred.strip() for pred in preds]\n","    output_data = {\n","            \"num_fail\": num_fail,\n","            \"scores\": scores,\n","            \"preds\": preds,\n","              \"labels\": targets}\n","    output_prediction_file = os.path.join(save_dir,\"predictions_ans_test.json\")\n","    with open(output_prediction_file, \"w\") as writer:\n","        writer.write(json.dumps(output_data, indent=4))\n","\n","# generate the rationale for the eval set\n","if args.prompt_format == \"QCM-LE\":\n","    torch.cuda.empty_cache()\n","    del predict_results, preds, targets\n","    predict_results = trainer.predict(test_dataset=eval_set, max_length=args.output_len) \n","    if trainer.is_world_process_zero():\n","        if args.use_generate:\n","            preds, targets = predict_results.predictions, predict_results.label_ids\n","        else:\n","            preds = predict_results.predictions[0]\n","            targets = predict_results.label_ids\n","            preds = preds.argmax(axis=2)\n","\n","        preds = tokenizer.batch_decode(\n","            preds, skip_special_tokens=True, clean_up_tokenization_spaces=True\n","        )\n","        targets = tokenizer.batch_decode(\n","            targets, skip_special_tokens=True, clean_up_tokenization_spaces=True\n","        )\n","        preds = [pred.strip() for pred in preds]\n","        output_data = {\"preds\": preds,\n","              \"labels\": targets}\n","        output_prediction_file = os.path.join(save_dir,\"predictions_ans_eval.json\")\n","        with open(output_prediction_file, \"w\") as writer:\n","            writer.write(json.dumps(output_data, indent=4))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3btYwbOip1id"},"outputs":[],"source":["actual_result = pretrain_trainer.predict(test_dataset=example_set, max_length=args.output_len)\n","\n","preds, targets = decode_pred(actual_result)\n","\n","preds[0]\n"]},{"cell_type":"code","source":["targets[0]"],"metadata":{"id":"_HG_NfYfp1EN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ii. With your now fully trained model, run inference on some data points! Do the rationales make sense? Do they help answer the question? Compare them to the rationales generated before the training.\n","\n","Play around with the model by modifying the example_set to run inference on the different values in the dataset. "],"metadata":{"id":"-efQhm60r-Tt"}},{"cell_type":"code","source":["example_qids = ['5'];\n","example_set = ScienceQADatasetImg(\n","    problems,\n","    example_qids,\n","    name_maps,\n","    tokenizer,\n","    args.input_len,\n","    args.output_len,\n","    args,\n","    image_features,\n","    args.test_le,\n",")\n","\n","actual_result = pretrain_trainer.predict(test_dataset=example_set, max_length=args.output_len)\n","rationale = actual_trainer_rationale.predict(test_dataset=example_set, max_length=args.output_len)\n"],"metadata":{"id":"0E4jIEx0p16M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preds_inference, targets_inference = decode_pred(actual_result)\n","preds_rationale, targets_rationale = decode_pred(rationale)\n","preds_inference[0]"],"metadata":{"id":"MPGSSwF2sgQv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preds_rationale[0]"],"metadata":{"id":"h3m2zViLtr22"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"19FdymmEqGXbvtW8A5mAN_iPx_IU1VvRm","timestamp":1682399329667},{"file_id":"1cPpsxAaOIk5ZsvQQfE7wPPq2ZeWXY9uu","timestamp":1682112189980}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}